{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkHEHL9WLDeu"
   },
   "source": [
    "# EJERCICIO 3. ENTRENAMIENTO DE MODELOS DE LENGUAJE PARA TRADUCCIÓN\n",
    "\n",
    "En este tercer ejercicio tendrás que trabajar con las técnicas vistas en los módulos 6 y 7, incluyendo los conceptos de tokenización, embedding, RNN y atención, para entrenar **dos modelos** de traducción del inglés al español.\n",
    "\n",
    "Para ello, vas a trabajar con el conjunto de datos `ageron/tatoeba_mt_train` de Datasets. Este dataset es el que se emplea en los capítulos 14 y 15 del libro [**Hands-On Machine Learning with Scikit-Learn and PyTorch**](https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991014210591904987).\n",
    "\n",
    "![](img/nmt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec_TFbA5LAs-"
   },
   "source": [
    "## 1. Enunciado\n",
    "\n",
    "Empleando el dataset `ageron/tatoeba_mt_train`, basándote en el libro [**Hands-On Machine Learning with Scikit-Learn and PyTorch**](https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991014210591904987) y usando su [código en github](https://github.com/ageron/handson-mlp), debes construir y probar **al menos 2** modelos con PyTorch:\n",
    "1. Un **modelo Encoder-Decoder basado en redes recurrentes** (`GRU` o `LSTM`). Reutilizando el código del capítulo 14 del libro (apartado *An Encoder-Decoder Network for Neural Machine Translation*), reproduce la red tipo encoder-decoder para traducción neuronal. Una vez hayas conseguido reproducir el entrenamiento del modelo, y puedas usar el modelo para traducir pequeñas frases, debes hacer tan solo uno de los siguientes cambios (a tu elección):\n",
    "* Usa otro tokenizador (en el mismo capítulo tienes otras variantes).\n",
    "* Incluye embeddings pre-entrenados solo en las entradas, tal y como se plantea al final del capítulo. No es necesario hacer la predicción el token a través del embedding, es suficiente con tener una representación dispersa (one-hot) para la predicción del siguiente token.\n",
    "* Modifica la arquitectura de la red, por ejemplo cambiando el `hidden_dim` de las unidades GRU, incluyendo `dropout` dentro de cada capa GRU (entre las dos layers que se incluyen), etc.\n",
    "\n",
    "2. Un **modelo Encoder-Decoder basado en transformers** (modelo del paper Attention is all you need). Reutilizando el código del capítulo 15 del libro (apartado *Building an English-to-Spanish Transformer*), reproduce el entrenamiento la red de tipo transformer para traducción neuronal. Una vez hayas conseguido reproducir el entrenamiento del modelo y usarlo para traducir pequeñas frases, debes hacer tan solo uno de los siguientes cambios (a tu elección):\n",
    "* Usa otro tokenizador (en el capítulo 14 tienes otras variantes).\n",
    "* Incluye embeddings pre-entrenados para los tokens de entrada. \n",
    "* Incluye la codificación posicional sinusuidal, tal y como se muestra al final del capítulo (sección \"Fixed Positional Encodings\").\n",
    "* Modifica la arquitectura de la red, aumentando el número de capas, incluyendo más dropout, etc. \n",
    "\n",
    "En tu reproducción del código del libro, comenta en código y/o celdas markdown lo que se está haciendo en cada caso (sin entrar en mucho detalle, pero demostrando que se entiende lo que hace).\n",
    "\n",
    "Finalmente, **elige** la mejor configuración a tu criterio, y prepara el **despliegue con PyTorch o con ONNXRuntime**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFfMGuPz4VDN"
   },
   "source": [
    "## 2. Entrega\n",
    "\n",
    "La entrega de este ejercicio se realiza a través de la tarea creada para tal efecto en Enseñanza Virtual. Tienes que entregar un **notebook**, y el **HTML** generado a partir de él, cuyas celdas estén ya evaluadas.\n",
    "\n",
    "La estructura del notebook debe contener los siguientes apartados:\n",
    "\n",
    "0. Cabecera: nombre y apellidos.\n",
    "1. Dataset: descripción, carga y visualización.\n",
    "2. Preparación de los datos para ser usados en PyTorch.\n",
    "3. Modelos y configuraciones creados en PyTorch (un sub-apartado para cada uno, explicando de forma razonada, con tus palabras y figuras, la modificación al código elegida). Entrenamiento y evaluación de cada modelo dentro de su sub-apartado para cada uno.\n",
    "4. Análisis de resultados.\n",
    "5. Elección del mejor modelo y despliegue con PyTorch o con ONNXRuntime para predecir nuevas frases.\n",
    "6. Bibliografía utilizada (libro, enlaces web, material de clase, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPR7zuh34VDO"
   },
   "source": [
    "### 2.1. Nota importante\n",
    "-----\n",
    "**HONESTIDAD ACADÉMICA Y COPIAS: un trabajo práctico es un examen, por lo que\n",
    "debe realizarse de manera individual. La discusión y el intercambio de\n",
    "información de carácter general con los compañeros se permite (e incluso se\n",
    "recomienda), pero NO AL NIVEL DE CÓDIGO. Igualmente el remitir código de\n",
    "terceros, OBTENIDO A TRAVÉS DE LA RED o cualquier otro medio, se considerará\n",
    "plagio.** \n",
    "\n",
    "**Cualquier plagio o compartición de código que se detecte significará\n",
    "automáticamente la calificación de CERO EN LA ASIGNATURA para TODOS los\n",
    "alumnos involucrados. Por tanto a estos alumnos NO se les conservará, para\n",
    "futuras convocatorias, ninguna nota que hubiesen obtenido hasta el momento.\n",
    "SIN PERJUICIO DE OTRAS MEDIDAS DE CARÁCTER DISCIPLINARIO QUE SE PUDIERAN\n",
    "TOMAR.**\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Ejercicio 3. Modelos para Traducción.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c3a869-d8d0-44b1-8c49-67efb1afbe61",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PRÁCTICA 3.1. INTRODUCCIÓN A PYTORCH\n",
    "\n",
    "Esta práctica está basada en el [tutorial](https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html) de Jeremy Howard (creador de fast.ai), recomendado para arrancar con PyTorch.\n",
    "\n",
    "Para esta primera práctica, asumiremos que ya has adquirido los fundamentos teóricos básicos de las redes neuronales artificiales, por lo que nos centraremos en ver cómo plasmar esos conocimientos en código ejecutable mediante PyTorch. \n",
    "\n",
    "PyTorch es un framework de código abierto para el aprendizaje profundo (Deep Learning). Destaca por su flexibilidad \"Pythónica\", su eficiencia en el cálculo de **tensores** (estructuras de datos similares a los arrays de NumPy pero optimizadas para GPU, como veremos en el siguiente módulo) y sus **grafos computacionales** dinámicos. Esta última característica simplifica la depuración y ofrece mayor libertad al construir modelos complejos. Es un proyecto de código abierto desarrollado dentro de la *Linux Foundation*, principalmente por *Meta AI* y grandes aportaciones de otras empresas como *NVIDIA*. \n",
    "\n",
    "## 1. ¿Cómo funciona y cómo se instala?\n",
    "\n",
    "PyTorch provee una serie de módulos y clases que son muy útiles para crear y entrenar redes neuronales, como por ejemplo: [torch.nn](https://pytorch.org/docs/stable/nn.html), [torch.optim](https://pytorch.org/docs/stable/optim.html), [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) y [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). Iremos construyendo sobre estas partes poco a poco para ver qué hace cada una.\n",
    "\n",
    "Pero antes, para instalar PyTorch, la forma más sencilla es visitar su página oficial (pytorch.org). Allí, selecciona tu sistema, gestor de paquetes (pip o conda), versión de Python y, lo más importante, si usarás CPU o GPU (CUDA/ROCm). La web te proporcionará el comando exacto. Si vas a usar una GPU con CUDA, se debe instalar una versión del driver de NVIDIA compatible con la GPU instalada, para ello consulta su [Compute Capability](https://developer.nvidia.com/cuda-gpus) y qué [versión del driver](https://docs.nvidia.com/datacenter/tesla/drivers/index.html) de CUDA lo soporta. Por ahora trabajaremos usando la CPU, y en el próximo módulo veremos cómo usar una GPU.\n",
    "\n",
    "Alternativamente, puedes usar plataformas en la nube con todo pre-instalado, como por ejemplo las siguientes opciones con versiones gratuitas:\n",
    "\n",
    "* *Google Colaboratory* (Colab): mejor opción para la mayoría de usuarios, experimentos rápidos, tutoriales, proyectos de portafolio y educación. Ofrece un entorno Jupyter Notebook totalmente gratuito que proporciona acceso a GPUs (Tesla T4, L4 o A100) y TPUs de Google. Se integra bien con Google Drive. La versión gratuita tiene un tiempo límite por sesión (12 horas), y el acceso a una GPU no está garantizado siempre. Existe una versión de pago (Colab Pro/Pro+) que ofrece más recursos y tiempos de ejecución más largos.\n",
    "* *Kaggle Kernels* (Notebooks de Kaggle): mejor para competiciones de ciencia de datos, aprendizaje, compartir código y colaborar. Ofrece un entorno de Jupyter Notebook que proporciona acceso gratuito a GPUs y CPUs. Kaggle es una plataforma excelente para encontrar datasets y aprender de la comunidad. Por supuesto, tiene una limitación similar a Colab para el tiempo de ejecución y recursos disponibles, aunque las especificaciones pueden variar. \n",
    "\n",
    "Una vez instalado, o usando una plataforma en la nube, podemos cargar PyTorch así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb28c3e7-823b-40a5-b9a3-05044c5af536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56ee2d8-9473-4ddd-814a-f9862fa1a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Veamos la versión que tenemos disponible\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b40279-f85f-4f67-b698-34b5d2365927",
   "metadata": {},
   "source": [
    "## 2. Tensores\n",
    "\n",
    "Antes de empezar a construir este primer ejemplo, veamos con más detenimiento el concepto de **tensor**. Un tensor es un contenedor de datos, usualmente números. Por ejemplo, una matriz es un tensor de 2 dimensiones. Los tensores son generalizaciones de las matrices a un número arbitrario de dimensiones (el nombre de tensor viene en realidad de la física). En el contexto de tensores, una dimensión se denomina eje (axis).\n",
    "\n",
    "Un tensor se define por 3 atributos principales:\n",
    "* Número de ejes o rango (**rank**). Por ejemplo, una matriz o tensor 2D itene 2 ejes.\n",
    "* Forma (**shape**). Una tupla de enteros que describen cuantas dimensiones tiene el tensor en cada eje. Un escalar tiene un shape igual a (). Una matriz de 3x4 tiene un shape (3, 4).\n",
    "* Tipo de dato (**dtype**). Es el tipo de dato contenido en el tensor, por ejemplo `float32` (número real de 32 bits), `uint8` (entero sin signo de 8 bits), `float16` (número real de media precisión, 16 bits), etc. Normalmente no hay tensores de cadenas de texto, ya que los tensores viven en segmentos de memoria pre-reservados y contiguos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b5e6-5b77-4432-a023-9f36ff33f4cb",
   "metadata": {},
   "source": [
    "Existen varios métodos básicos para crear tensores de un tamaño personalizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968fd440-b520-4f9b-ac1e-3b7e8c7305a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3948, 0.3103, 0.5273],\n",
      "        [0.5953, 0.0451, 0.9951]])\n",
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor con ceros\n",
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "# tensor con unos\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "# tensor inicializado aleatoriamente. \n",
    "# Podemos fijar una semilla para que simpre sean los mismos:\n",
    "# torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)\n",
    "\n",
    "# crea un tensor con valores predefinidos mediante listas\n",
    "constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(constants)\n",
    "\n",
    "# existen versiones de los métodos anteriores pero con _like\n",
    "# para indicar que queremos un tensor con la misma forma que otro\n",
    "constants_zeros = torch.zeros_like(constants)\n",
    "constants.shape == constants_zeros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d1eec-ba36-4041-8bb2-a421653d6b95",
   "metadata": {},
   "source": [
    "**Ejercicio:** crea a continuación un tensor con rango 3 y forma (2,3,2), y observa cómo se distribuyen los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf740f6-4dc0-48f4-9d43-cddb212ef3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60bf93e0-7a26-4499-a2f3-00aacd8e529b",
   "metadata": {},
   "source": [
    "También podemos hacer operaciones aritméticas y lógicas con tensores y escalares, y con tensores y tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e626d56a-f38c-4146-9b4e-623defb3850f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]])\n",
      "tensor([[1.4142, 1.4142, 1.4142],\n",
      "        [1.4142, 1.4142, 1.4142]])\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([[12., 12., 12.],\n",
      "        [12., 12., 12.]])\n",
      "tensor([[ 2.,  4.,  8.],\n",
      "        [16., 32., 64.]])\n"
     ]
    }
   ],
   "source": [
    "# Sumar un escalar\n",
    "ones = torch.zeros(2, 3) + 1\n",
    "# Multiplicar un escalar\n",
    "twos = torch.ones(2, 3) * 2\n",
    "# Combinación\n",
    "threes = (torch.ones(2, 3) * 7 - 1) / 2\n",
    "# Potencias\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)\n",
    "\n",
    "# Suma de tensores\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "# Traspuesta de un tensor\n",
    "twos_T = twos.T\n",
    "print (twos_T)\n",
    "\n",
    "# Multiplicación matricial\n",
    "sixes = ones @ twos_T\n",
    "print (sixes)\n",
    "\n",
    "# Multiplicación elemento a elemento\n",
    "dozens = threes * fours\n",
    "print(dozens)\n",
    "\n",
    "# Potencia con tensores (por elemento)\n",
    "powers2 = twos ** torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(powers2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becd81e-ee70-4f6a-8e31-55beb1db5b2d",
   "metadata": {},
   "source": [
    "## 3. Descarga de los datos MNIST\n",
    "\n",
    "Para este primer ejemplo, vamos a usar el dataset clásico [MNIST](https://yann.lecun.com/exdb/mnist/index.html), que consiste en 70.000 imágenes en blanco y negro de dígitos escritos a mano (entre 0 y 9). \n",
    "\n",
    "Usaremos [pathlib](https://docs.python.org/3/library/pathlib.html) para usar rutas y la descarga la haremos con [requests](http://docs.python-requests.org/en/master/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96df725-5251-41ae-acd3-598350dab1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)  # creamos la carpeta\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists(): # evitamos descargar si ya se ha descargado\n",
    "        content = requests.get(URL + FILENAME).content  # descargamos el fichero\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f80a3-5cfb-408f-8340-5336dea61496",
   "metadata": {},
   "source": [
    "Este dataset está en formato **array de numpy**, y se ha guardado usando **pickle**, un formato específico de python para serializar datos. Normalmente usamos `X` para denominar los ejemplos (en este caso, las imágenes), e `Y` para las etiquetas (los números a los que corresponde cada imagen). El dataset también viene ya particionado en 50.000 imágenes para train, 10.000 para validación y 10.000 para test. A continuación descomprimimos el dataset (estaba en formato .gz) y de-serializamos los datos solo para train y valid (por ahora dejaremos fuera a test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fead0c3d-4f6b-446c-975f-cfc43567a5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:  # descomprimimos el fichero en f\n",
    "        ((x_train_np, y_train_np), (x_valid_np, y_valid_np), _) = pickle.load(f, encoding=\"latin-1\") # cargamos los datos serializados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7106c8-c4d1-47fb-8794-8dc61112c6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df8ad94-ab38-4793-ac58-09b833e733dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b5734-7899-4e81-b1db-2f5bb648577e",
   "metadata": {},
   "source": [
    "Como puedes ver, hemos cargado una matriz de 50.000 filas (cada imagen) con 784 columnas (pixeles de cada imagen). Esto es así porque cada imagen es una matriz de 28 x 28 pixeles, y se almacena de forma aplanada (`flattened`, es decir, por filas) con 28x28=784 elementos en total. Veamos la primera imagen (fila 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85268540-2b9a-42d2-aa5f-7a3c02c2093a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "# Necesitamos cambiar la forma de la matriz a 28x28\n",
    "pyplot.imshow(x_train_np[0].reshape((28, 28)), cmap=\"gray\")\n",
    "try:\n",
    "    import google.colab  # si estamos en google colab, no es necesario la siguiente línea\n",
    "except ImportError:\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e05a0-de44-4485-b5a0-30ca4d581203",
   "metadata": {},
   "source": [
    "El número mostrado en la imagen parece un 5, comprobemos que es así consultando su etiqueta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154cb201-130d-458f-a7bb-ddcfa00b1a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_np[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63a217-3f0c-4e63-ba63-4010a334d856",
   "metadata": {},
   "source": [
    "**Ejercicio:** Prueba a visualizar otras imágenes del conjunto de entrenamiento, así como del conjunto de validación (`x_valid`), en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cbc6aa8-c5a7-4e0e-b6b4-02beea45dda6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número corresponde a un  tensor(5)\n"
     ]
    }
   ],
   "source": [
    "## EJERCICIO\n",
    "\n",
    "x_ej1 = x_train_np  # 1. cambia x_train_np por x_valid_np\n",
    "y_ej1 = y_train_np  # 2. cambia y_train_np por y_valid_np\n",
    "img_ej1 = 100    # 3. cambia el número por otro\n",
    "\n",
    "pyplot.imshow(x_ej1[img_ej1].reshape((28, 28)), cmap=\"gray\") \n",
    "try:\n",
    "    import google.colab \n",
    "except ImportError:\n",
    "    pyplot.show()\n",
    "    \n",
    "print('El número corresponde a un ', y_train[img_ej1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91bed4-7f36-48d7-9509-615142779cfc",
   "metadata": {},
   "source": [
    "PyTorch usa para los tensores el tipo de dato `torch.tensor` en vez de arrays de numpy, así que necesitamos convertir los datos a este formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398edd2e-d038-46b8-a834-13727b59d310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# map equivale a realizar un bucle que recorra los elementos\n",
    "x_train, y_train, x_valid, y_valid = map(  \n",
    "    torch.tensor, (x_train_np, y_train_np, x_valid_np, y_valid_np)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05eca98b-17ba-420a-a62f-00b9ce5833dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([50000, 784])\n",
      "torch.float32\n",
      "tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "El valor mínimo y máximo de las etiquetas son  tensor(0)  y  9\n"
     ]
    }
   ],
   "source": [
    "n, c = x_train.shape\n",
    "# previsualizamos el tensor x_train (no será completo)\n",
    "print(x_train)\n",
    "# La forma del tensor\n",
    "print(x_train.shape)\n",
    "# El tipo de dato en el tensor\n",
    "print(x_train.dtype)\n",
    "# Previsualizamos Y_train\n",
    "print(y_train)\n",
    "# Con .item() sacamos el valor de un tensor (mira la diferencia entre min y max)\n",
    "print('El valor mínimo y máximo de las etiquetas son ',y_train.min(), ' y ', y_train.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47eac05-8130-4682-93dd-4dc41653a281",
   "metadata": {},
   "source": [
    "## 4. Primera red neuronal hecha a mano\n",
    "\n",
    "### 4.1. Definición del modelo\n",
    "Vamos a hacer un primer modelo lineal en Pytorch con simples operaciones tensoriales, usando tensores para representar los pesos `weights` y el bias `bias`: `y_hat = X*weights+bias`, donde `y_hat` será la predicción de la variable `Y`.\n",
    "\n",
    "Ahora vamos a añadir algo nuevo, vamos a indicar a PyTorch hay que calcular gradientes en los tensores, con `requires_grad`. Esto hará que PyTorch grabe todas las operaciones que se hagan sobre ese tensor, para que después podamos hacer la retro-propagación de forma automática. Observa a continuación que:\n",
    "\n",
    "* para los pesos, indicamos `requires_grad` después de inicializarlo, ya que no queremos que ese paso (la división por la raíz cuadrada de 784) se registre en el gradiente. En PyTorch, añadir un `_` al final del nombre de la función indica que la operación modifica los valores de la variable.\n",
    "* para el bias sí que podemos indicar `requires_grad` en el momento de inicializarlo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c834b80-3484-4cb8-9fc4-22a9f66c62bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab037b9-0902-41bf-92cd-a7e4cdee26c9",
   "metadata": {},
   "source": [
    "**Ejercicio**: ¿Qué tipo de inicialización estamos haciendo a los pesos?\n",
    "<details>\n",
    "<summary>Click aquí para ver solución</summary>\n",
    "    Estamos haciendo la inicialización Xavier, ya que multiplicamos por `1/sqrt(n)`, con `n` el número de conexiones de entrada.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6b089-2568-47d0-9626-90bff60c9b37",
   "metadata": {},
   "source": [
    "### 4.2. Función de activación en capa de salida\n",
    "PyTorch se integra perfectamente con Python, de tal forma que podemos usar cualquier función estándar de este lenguaje (o un objeto \"callable\") como un modelo, ya que los gradientes se computan automáticamente. Teniendo esto en cuenta, vamos a definir:\n",
    "* la función de activación **[log softmax](https://www.baeldung.com/cs/softmax-vs-log-softmax)**, que es una variante de **softmax** pero con una mejor estabilidad numérica (en cuestión de precisión decimal). La fórmula la podemos escribir:\n",
    "$$\n",
    "\\text{log\\_softmax}(x_i) = \\log(\\text{softmax}(x_i)) = \\log\\left(\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\right) = x_i - \\log\\left(\\sum_j \\exp(x_j)\\right)\n",
    "$$\n",
    "Otra forma de verlo es como `log_softmax (x_i) = x_i - log(sum(exp(x_j)))`. Es decir, para cada valor `x_i` le restamos el log de la suma de todos los valores x_j. Aunque estas funciones de activación ya existen en PyTorch, esto demuestra que implementarlos a mano también es posible y es eficiente, ya que automáticamente se creará una versión compilable.\n",
    "* el modelo lineal es la multiplicación de matrices (operador `@`) entre la entrada y los pesos, más el bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e24559ea-8801-436e-b0dd-b383ec67de73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9cbad-0da6-45aa-aa2d-f87eab879978",
   "metadata": {},
   "source": [
    "**Ejercicio opcional**: Puedes entender cómo se implementa log_softmax con `x.exp().sum(-1).log().unsqueeze(-1)` si vas añadiendo cada operación una a una sobre un tensor simple. Ejecútalo a continuación y comenta al lado de cada línea que ha hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f07c9888-5e35-49df-aa82-7803f2d298b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2]])\n",
      "tensor([[2.7183, 2.7183, 2.7183],\n",
      "        [7.3891, 7.3891, 7.3891]])\n",
      "tensor([ 8.1548, 22.1672])\n",
      "tensor([2.0986, 3.0986])\n",
      "tensor([[2.0986],\n",
      "        [3.0986]])\n"
     ]
    }
   ],
   "source": [
    "## EJERCICIO\n",
    "sample = torch.tensor([[1,1,1],[2,2,2]])\n",
    "\n",
    "print(sample)  # el vector de entrada\n",
    "print(sample.exp())   # Aplica e^x para cada valor x en samples\n",
    "print(sample.exp().sum(-1))   # ...\n",
    "print(sample.exp().sum(-1).log())   # ...\n",
    "print(sample.exp().sum(-1).log().unsqueeze(-1))    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59d5f0-78e3-451f-b363-3ec66e5e1a40",
   "metadata": {},
   "source": [
    "Vamos a usar el modelo para hacer inferencia sobre un batch de 64 elementos. Por ahora, las predicciones serán aleatorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "627fce7a-85ba-49ee-964b-567d30d49c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4225, -2.6129, -2.4560, -2.0440, -1.8490, -2.6357, -2.2152, -2.4246,\n",
      "        -2.2567, -2.3937], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "batch = 64  # batch size\n",
    "\n",
    "xb = x_train[0:batch]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbfdf5-8865-4c81-9ac6-0f2ee99f7262",
   "metadata": {},
   "source": [
    "El tensor resultado de la predicción, `preds`, contiene también una función de gradiente (`grad_fn`). Esta función se utilizará para realizar la retropropagación.\n",
    "\n",
    "### 4.3. Función de pérdida y métrica de evaluación\n",
    "\n",
    "A continuación vamos a implementar la función de pérdida *logaritmo-verosimilitud negativa* (NLL, o negative log-likelihood), la cual se deriva fácilmente de la fórmula vista en clase para clasificación multiclase, asumiendo que solo puede haber una clase válida para cada ejemplo. La fórmula es:\n",
    "\n",
    "$$\n",
    "\\ell(x, y) = \\text{media}(\\{l_1, \\dots, l_N\\}), \\quad l_i = -x_{i,y_i}\n",
    "$$\n",
    "\n",
    "donde\n",
    "* N es el número de ejemplos (e.g. tamaño del batch)\n",
    "* y es un tensor con N elementos, con valores entre 0 y C-1.\n",
    "* x es un tensor de rango 2, con N filas y C columnas, donde:\n",
    "    * cada fila es el valor predicho por el modelo para cada ejemplo, que consiste en un vector de C valores, uno por cada clase.\n",
    "    * asumimos que **a cada fila de x le hemos aplicado log_softmax**.\n",
    "    * x_{i,j} es el valor en la fila i y columna j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d3e7545-1995-4a88-b2f3-ff91f8afdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b435438-70c5-461b-8370-bca1fc9bda76",
   "metadata": {},
   "source": [
    "Hagamos una prueba rápida con el primer batch. Esto mejorará cuando entrenemos el modelo, recuerda que buscaremos reducir el valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4446699-deec-4ff4-9325-c49e280b40e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3143, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:batch]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d616c82-f2f2-4659-87ce-cc72dd74ab69",
   "metadata": {},
   "source": [
    "En PyTorch tenemos que definir nuestra propia función de accuracy, el cual nos dará un valor más interpetable para nosotros, pero que no serviría para entrenar un modelo. Será la suma ponderada de las veces que el modelo acierta a la hora de predecir la clase de un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6d277a4-ac0e-4a2f-a639-ebabbab7b40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b6d63-a347-4d72-9cf3-c7848dd97b63",
   "metadata": {},
   "source": [
    "Efectivamente, al ser un modelo aleatorio, y al haber 10 clases, esperamos obtener alrededor de un 1/10 = 0.10 = 10% de precisión. Esto se conoce como el \"random baseline\". Vamos a mejorarlo entrenando el modelo.\n",
    "\n",
    "### 4.4. Bucle de entrenamiento\n",
    "A continuación verás un bucle donde aplicaremos las épocas, de la siguiente forma:\n",
    "* seleccionamos un mini-batch de tamaño `bs`\n",
    "* usamos el modelo para hacer predicciones\n",
    "* calculamos la pérdida\n",
    "* `loss.backward()` actualizará los gradientes del modelo (de `weights` y de `bias`). Si te lo preguntas, la función loss sabe cómo hacerlo porque lo hace a través de las predicciones que el modelo ha generado, cuyos gradientes ya vienen enlazados con los parámetros del modelo (como dijimos al comenzar esta sección).\n",
    "* Usamos los gradientes para actualizar los parámetros (`weights` y `bias`). Est aactualización no debe registrarse como operaciones a registrar para propagación de gradientes, por lo que usamos el contexto `torch.no_grad()`.\n",
    "* Finalmente, reiniciamos los gradientes a cero con `.grad.zero_()` para la siguiente iteración. Si no se hace esto, estaríamos siempre acumulando los gradientes entre las iteraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "960a3d1f-0568-4258-bade-0a778921076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # cuántas épocas\n",
    "\n",
    "for epoch in range(epochs):  # bucle para cada época\n",
    "    for i in range((n - 1) // bs + 1):   # bucle para cada mini-batch\n",
    "        #         set_trace()\n",
    "        start_i = i * bs       # donde empieza el mini-batch\n",
    "        end_i = start_i + bs   # donde finaliza el mini-batch\n",
    "        xb = x_train[start_i:end_i]  # extraemos ejemplos\n",
    "        yb = y_train[start_i:end_i]  # extraemos etiquetas\n",
    "        pred = model(xb)             # el modelo genera predicciones\n",
    "        loss = loss_func(pred, yb)   # calculamos la pérdida\n",
    "\n",
    "        loss.backward()              # retro-propagamos los gradientes\n",
    "        with torch.no_grad():        # actualizamos pesos (no registramos gradientes!)\n",
    "            weights -= weights.grad * lr    # regla de actualización simple, sin momentum\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()     # reseteamos los gradientes a cero\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17f01b6c-6b87-466b-9f28-ee7640c0d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0804, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99936fd2-8560-4098-8418-a8581260c96a",
   "metadata": {},
   "source": [
    "Vemos como han mejorado los valores, la pérdida ha bajado y el modelo obtiene una precisión perfecta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c1486-d675-4b73-8b26-34c87713da30",
   "metadata": {},
   "source": [
    "## 5. Creación de la red neuronal con elementos de PyTorch\n",
    "\n",
    "Ahora vamos a simplificar el código anterior significativamente usando elementos que ya nos proporciona PyTorch.\n",
    "\n",
    "nn.Module\n",
    "\n",
    "nn.linear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

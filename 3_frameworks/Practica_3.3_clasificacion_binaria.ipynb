{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelamda/DL/blob/master/3.%20Frameworks%20Software/Practica3.4.%20Keras%3A%20clasificacion%20binaria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gERN02cprk5K"
      },
      "source": [
        "# PRÁCTICA 3.3. CLASIFICACIÓN BINARIA Y DATASETS DESBALANCEADOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzU7HoRBrk5L"
      },
      "source": [
        "La clasificación binaria es, posiblemente, el tipo de problema con mayor número de aplicaciones en ML. En esta práctica vamos a ver cómo construir un clasificador binario en PyTorch, cómo trabajar con un dataset tabular y desbalanceado, y cómo trabajar con otras métricas de calidad. Concretamente, construiremos una red neuronal que nos ayude a clasificar si un movimiento en una tarjeta de crédito es fraudulento, mediante el dataset [*Credit Card Fraud Detection*](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).\n",
        "\n",
        "Comencemos importando PyTorch y comprobando si tenemos GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hfhw4QYlrk5K",
        "outputId": "1649c6b2-cb0a-4c10-c7d9-c1284efd782e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.3.0+cu121 \n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('PyTorch version:', torch.__version__, '\\nDevice:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preparación del dataset\n",
        "\n",
        "### 1.1. Descarga\n",
        "\n",
        "A continuación puedes descargar los datos y descomprimirlo con el siguiente código. Si el enlace de descarga ha dejado de funcionar, puedes descargarlo a mano desde [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), o bien usando Kagglehub como se indica en la web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "DATA_PATH = Path(\"./data\")\n",
        "PATH = DATA_PATH / \"creditcard\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)  # creamos la carpeta\n",
        "\n",
        "URL = \"https://clouda-labs-assets.s3-us-west-2.amazonaws.com/fraud-detection/\"\n",
        "FILENAME = \"creditcard.csv.zip\"\n",
        "\n",
        "if not (PATH / FILENAME).exists(): # evitamos descargar si ya se ha descargado\n",
        "        content = requests.get(URL + FILENAME).content  # descargamos el fichero\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)\n",
        "\n",
        "        with zipfile.ZipFile((PATH / FILENAME), 'r') as zip_ref: # descomprimimos el fichero\n",
        "                zip_ref.extract(\"creditcard.csv\", PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprueba a continuación que puedes abrir correctamente el fichero con `Pandas` (si no tienes esta librería, te aconsejo instalarla con `pip install pandas`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Primeras 5 filas del dataset ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(PATH / \"creditcard.csv\")\n",
        "print(\"--- Primeras 5 filas del dataset ---\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Exploración de los datos\n",
        "\n",
        "El paso más importante a la hora de entrar cualquier modelo de machine learning, es entender el dataset y prepararlo adecuadamente para el modelo. Todo comienza por los datos, y por muy potente que sea un modelo, si los datos no tienen una buena calidad, conseguiremos resultados muy pobres.\n",
        "\n",
        "A continuación podemos ver que efectivamente, el dataset está muy desbalanceado. Esto nos llevará a conseguir un modelo muy sesgado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Información del DataFrame ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "\n",
            "--- Distribución de Clases (Fraude vs. No Fraude) ---\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "El dataset está muy desbalanceado: solo un 0.1727% de las transacciones son fraudulentas.\n"
          ]
        }
      ],
      "source": [
        "# Mostrar las primeras filas y la información general\n",
        "print(\"\\n--- Información del DataFrame ---\")\n",
        "df.info()\n",
        "\n",
        "# --- Visualización del desbalance de clases ---\n",
        "print(\"\\n--- Distribución de Clases (Fraude vs. No Fraude) ---\")\n",
        "class_counts = df['Class'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Gráfico para visualizar el desbalance\n",
        "df['Class'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribución de Clases\\n (0: No Fraude || 1: Fraude)')\n",
        "plt.xlabel('Clase')\n",
        "plt.ylabel('Cantidad')\n",
        "plt.show()\n",
        "\n",
        "# Imprimir el porcentaje de fraudes\n",
        "fraud_percentage = (class_counts[1] / class_counts.sum()) * 100\n",
        "print(f\"\\nEl dataset está muy desbalanceado: solo un {fraud_percentage:.4f}% de las transacciones son fraudulentas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Análisis y preparación de los datos\n",
        "\n",
        "Antes de entrar a trabajar con los datos, hablemos antes sobre el preprocesamiento de datos tabulares (tablas) en Deep Learning. Esto es algo crítico y que tendrá un gran efecto sobre el entrenamiento de redes neuronales, y es que es esencial preparar los datos con dos transformaciones principales según el tipo de columna (variable):\n",
        "1. **Codificación de variables categóricas (One-Hot Encoding)**: convierte una variables categórica (por ejemplo, color) en un formato numérico que el modelo pueda entender, creando una nueva variable *dummy* binaria (0 o 1) para cada categoría (por ejemplo, rojo, azul y verde). Por ejemplo, rojo se convierte en un vector [1, 0, 0], azul [0, 1, 0] y verde [0, 0, 1]. Esto evita que el modelo interprete las categorías como si tuvieran un orden o valor numérico, siendo \"equidistantes\" entre sí.\n",
        "\n",
        "2. **Escalado de variables numéricas**: asegura que las variables con diferentes rangos y varianzas no dominen el entrenamiento. Esto previene que el modelo se vea sesgado por valores muy grandes o pequeños, mejorando su convergencia y rendimiento (por ejemplo, si en datos de viviendas, tenemos una variable con el tamaño en metros cuadrados y otra con el precio en euros). Hay dos métodos principales:  \n",
        "  * **Normalización Min-Max (MinMaxScaler)**: Escala los datos para que se encuentren en un rango específico, generalmente [0, 1]. Útil cuando no conocemos la distribución de los datos, por ejemplo, en los valores de los píxeles de una imagen.\n",
        "  * **Estandarización Z-Score (StandardScaler)**: Centra los datos a una media de 0 y una desviación estándar de 1. Es útil en algoritmos basados en gradiente o en reducción de dimensionalidad.\n",
        "\n",
        "En nuestro caso, para proteger la privacidad de los usuarios y por cuestiones de confidencialidad, los creadores del dataset aplicaron una transformación matemática llamada *Análisis de Componentes Principales (PCA)* a los datos originales. Uno de los pasos estándar que se realizan antes de aplicar PCA es escalar los datos (por ejemplo, usando StandardScaler). Por lo tanto, las columnas `V` que recibimos en el dataset ya han sido transformadas y se encuentran en una escala comparable, centradas alrededor de cero. Puedes observar a continuación que las medias de estas variables son valores muy pequeños, cercanos al cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>284807.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94813.859575</td>\n",
              "      <td>1.168375e-15</td>\n",
              "      <td>3.416908e-16</td>\n",
              "      <td>-1.379537e-15</td>\n",
              "      <td>2.074095e-15</td>\n",
              "      <td>9.604066e-16</td>\n",
              "      <td>1.487313e-15</td>\n",
              "      <td>-5.556467e-16</td>\n",
              "      <td>1.213481e-16</td>\n",
              "      <td>-2.406331e-15</td>\n",
              "      <td>...</td>\n",
              "      <td>1.654067e-16</td>\n",
              "      <td>-3.568593e-16</td>\n",
              "      <td>2.578648e-16</td>\n",
              "      <td>4.473266e-15</td>\n",
              "      <td>5.340915e-16</td>\n",
              "      <td>1.683437e-15</td>\n",
              "      <td>-3.660091e-16</td>\n",
              "      <td>-1.227390e-16</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>0.001727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "      <td>1.516255e+00</td>\n",
              "      <td>1.415869e+00</td>\n",
              "      <td>1.380247e+00</td>\n",
              "      <td>1.332271e+00</td>\n",
              "      <td>1.237094e+00</td>\n",
              "      <td>1.194353e+00</td>\n",
              "      <td>1.098632e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>7.345240e-01</td>\n",
              "      <td>7.257016e-01</td>\n",
              "      <td>6.244603e-01</td>\n",
              "      <td>6.056471e-01</td>\n",
              "      <td>5.212781e-01</td>\n",
              "      <td>4.822270e-01</td>\n",
              "      <td>4.036325e-01</td>\n",
              "      <td>3.300833e-01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.041527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "      <td>-4.832559e+01</td>\n",
              "      <td>-5.683171e+00</td>\n",
              "      <td>-1.137433e+02</td>\n",
              "      <td>-2.616051e+01</td>\n",
              "      <td>-4.355724e+01</td>\n",
              "      <td>-7.321672e+01</td>\n",
              "      <td>-1.343407e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.483038e+01</td>\n",
              "      <td>-1.093314e+01</td>\n",
              "      <td>-4.480774e+01</td>\n",
              "      <td>-2.836627e+00</td>\n",
              "      <td>-1.029540e+01</td>\n",
              "      <td>-2.604551e+00</td>\n",
              "      <td>-2.256568e+01</td>\n",
              "      <td>-1.543008e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "      <td>-8.903648e-01</td>\n",
              "      <td>-8.486401e-01</td>\n",
              "      <td>-6.915971e-01</td>\n",
              "      <td>-7.682956e-01</td>\n",
              "      <td>-5.540759e-01</td>\n",
              "      <td>-2.086297e-01</td>\n",
              "      <td>-6.430976e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.283949e-01</td>\n",
              "      <td>-5.423504e-01</td>\n",
              "      <td>-1.618463e-01</td>\n",
              "      <td>-3.545861e-01</td>\n",
              "      <td>-3.171451e-01</td>\n",
              "      <td>-3.269839e-01</td>\n",
              "      <td>-7.083953e-02</td>\n",
              "      <td>-5.295979e-02</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "      <td>1.798463e-01</td>\n",
              "      <td>-1.984653e-02</td>\n",
              "      <td>-5.433583e-02</td>\n",
              "      <td>-2.741871e-01</td>\n",
              "      <td>4.010308e-02</td>\n",
              "      <td>2.235804e-02</td>\n",
              "      <td>-5.142873e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.945017e-02</td>\n",
              "      <td>6.781943e-03</td>\n",
              "      <td>-1.119293e-02</td>\n",
              "      <td>4.097606e-02</td>\n",
              "      <td>1.659350e-02</td>\n",
              "      <td>-5.213911e-02</td>\n",
              "      <td>1.342146e-03</td>\n",
              "      <td>1.124383e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "      <td>1.027196e+00</td>\n",
              "      <td>7.433413e-01</td>\n",
              "      <td>6.119264e-01</td>\n",
              "      <td>3.985649e-01</td>\n",
              "      <td>5.704361e-01</td>\n",
              "      <td>3.273459e-01</td>\n",
              "      <td>5.971390e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.863772e-01</td>\n",
              "      <td>5.285536e-01</td>\n",
              "      <td>1.476421e-01</td>\n",
              "      <td>4.395266e-01</td>\n",
              "      <td>3.507156e-01</td>\n",
              "      <td>2.409522e-01</td>\n",
              "      <td>9.104512e-02</td>\n",
              "      <td>7.827995e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "      <td>9.382558e+00</td>\n",
              "      <td>1.687534e+01</td>\n",
              "      <td>3.480167e+01</td>\n",
              "      <td>7.330163e+01</td>\n",
              "      <td>1.205895e+02</td>\n",
              "      <td>2.000721e+01</td>\n",
              "      <td>1.559499e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.720284e+01</td>\n",
              "      <td>1.050309e+01</td>\n",
              "      <td>2.252841e+01</td>\n",
              "      <td>4.584549e+00</td>\n",
              "      <td>7.519589e+00</td>\n",
              "      <td>3.517346e+00</td>\n",
              "      <td>3.161220e+01</td>\n",
              "      <td>3.384781e+01</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time            V1            V2            V3            V4  \\\n",
              "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
              "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
              "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
              "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
              "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
              "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
              "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
              "\n",
              "                 V5            V6            V7            V8            V9  \\\n",
              "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
              "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
              "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
              "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
              "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
              "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
              "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
              "\n",
              "       ...           V21           V22           V23           V24  \\\n",
              "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
              "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
              "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
              "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
              "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
              "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
              "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
              "\n",
              "                V25           V26           V27           V28         Amount  \\\n",
              "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
              "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
              "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
              "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
              "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
              "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
              "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
              "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
              "\n",
              "               Class  \n",
              "count  284807.000000  \n",
              "mean        0.001727  \n",
              "std         0.041527  \n",
              "min         0.000000  \n",
              "25%         0.000000  \n",
              "50%         0.000000  \n",
              "75%         0.000000  \n",
              "max         1.000000  \n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, las columnas `Amount` y `Time` son las originales, sin haber sido procesadas con PCA. Sus valores numéricos son tan grandes que dominarían el proceso de aprendizaje de la red neuronal. El modelo le daría muchísima más importancia a una variación en Amount (que puede ser de miles) que a una variación en V1 (que puede ser de 0.1), simplemente por la diferencia de magnitud. \n",
        "\n",
        "A continuación haremos uso de `StandardScaler`, una funcionalidad de `sklearn` (si no la tienes instalada, te aconsejo instalarla con `pip install scikit-learn`) en `Time` y `Amount`, las ponemos en la misma \"escala\" que las variables V. Esto asegura que el modelo pueda aprender de todas las características de manera justa, sin que ninguna domine a las demás por su magnitud numérica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scaled_Amount</th>\n",
              "      <th>Amount</th>\n",
              "      <th>scaled_Time</th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.913952e-17</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>-3.065637e-16</td>\n",
              "      <td>94813.859575</td>\n",
              "      <td>1.168375e-15</td>\n",
              "      <td>3.416908e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000002e+00</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>1.000002e+00</td>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.532294e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.996583e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-3.308401e-01</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>-8.552120e-01</td>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-2.652715e-01</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>-2.131453e-01</td>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-4.471707e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>9.372174e-01</td>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.023622e+02</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.642058e+00</td>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       scaled_Amount         Amount   scaled_Time           Time  \\\n",
              "count   2.848070e+05  284807.000000  2.848070e+05  284807.000000   \n",
              "mean    2.913952e-17      88.349619 -3.065637e-16   94813.859575   \n",
              "std     1.000002e+00     250.120109  1.000002e+00   47488.145955   \n",
              "min    -3.532294e-01       0.000000 -1.996583e+00       0.000000   \n",
              "25%    -3.308401e-01       5.600000 -8.552120e-01   54201.500000   \n",
              "50%    -2.652715e-01      22.000000 -2.131453e-01   84692.000000   \n",
              "75%    -4.471707e-02      77.165000  9.372174e-01  139320.500000   \n",
              "max     1.023622e+02   25691.160000  1.642058e+00  172792.000000   \n",
              "\n",
              "                 V1            V2  \n",
              "count  2.848070e+05  2.848070e+05  \n",
              "mean   1.168375e-15  3.416908e-16  \n",
              "std    1.958696e+00  1.651309e+00  \n",
              "min   -5.640751e+01 -7.271573e+01  \n",
              "25%   -9.203734e-01 -5.985499e-01  \n",
              "50%    1.810880e-02  6.548556e-02  \n",
              "75%    1.315642e+00  8.037239e-01  \n",
              "max    2.454930e+00  2.205773e+01  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df['scaled_Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
        "df['scaled_Time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
        "df[['scaled_Amount','Amount','scaled_Time','Time','V1','V2']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por otro lado, podemos observar que las transacciones fraudulentas no tienden a suponer una gran cantidad de dinero (`Amount`), como se observa a continuación (observa la media y el máximo, y compáralo con la clase negativa):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Estadísticas de transacciones negativas (no fraufulentas) ---\n",
            "count    284315.000000\n",
            "mean         88.291022\n",
            "std         250.105092\n",
            "min           0.000000\n",
            "25%           5.650000\n",
            "50%          22.000000\n",
            "75%          77.050000\n",
            "max       25691.160000\n",
            "Name: Amount, dtype: float64\n",
            "\n",
            "--- Estadísticas de transacciones positivas (fraudulentas) ---\n",
            "count     492.000000\n",
            "mean      122.211321\n",
            "std       256.683288\n",
            "min         0.000000\n",
            "25%         1.000000\n",
            "50%         9.250000\n",
            "75%       105.890000\n",
            "max      2125.870000\n",
            "Name: Amount, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'whiskers': [<matplotlib.lines.Line2D at 0x7f3384568250>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33845683a0>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33844eb370>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33844eb4c0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f3384568640>,\n",
              "  <matplotlib.lines.Line2D at 0x7f3384568b80>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33844eb760>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33d77aa340>],\n",
              " 'boxes': [<matplotlib.patches.PathPatch at 0x7f3384563c40>,\n",
              "  <matplotlib.patches.PathPatch at 0x7f3384568e50>],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f3384568e20>,\n",
              "  <matplotlib.lines.Line2D at 0x7f33d77aa7c0>],\n",
              " 'fliers': [],\n",
              " 'means': []}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"--- Estadísticas de transacciones negativas (no fraufulentas) ---\")\n",
        "print(df[df[\"Class\"] == 0][\"Amount\"].describe())\n",
        "print(\"\\n--- Estadísticas de transacciones positivas (fraudulentas) ---\")\n",
        "print(df[df[\"Class\"] == 1][\"Amount\"].describe())\n",
        "plt.boxplot([df[df['Class']==0]['Amount'],df[df['Class']==1]['Amount']],\n",
        "            patch_artist=True,labels=['no fraude','fraude'],\n",
        "            showfliers=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminamos las columnas originales de 'Time' y 'Amount'\n",
        "# Ya no nos hacen falta para entrenar el modelo\n",
        "df.drop(['Time', 'Amount'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder evaluar cómo va el progreso del entrenamiento, vamos a dividir el dataset en tres:\n",
        "* _Conjunto de entrenamiento (train)_: con el que intentaremos optimizar los pesos de la red para que minimice la función de pérdida. \n",
        "\n",
        "* _Conjunto de validación (validation)_: con el que se medirá cómo de bueno es el modelo concreto que estamos entrenando (con unos parámetros fijos). Es algo así como un conjunto de test temporal. \n",
        "\n",
        "* _Conjunto de prueba (test)_: que no se ha usado en ningún momento de las iteraciones anteriores y que permite medir de forma objetiva la bondad del modelo final obtenido.\n",
        "\n",
        "A continuación preparamos las características X y la etiqueta objetivo Y, y partimos el dataset en tres de forma estratificada. Esto es, ya que tenemos una gran desbalance, obligar a que tengamos una proporción igual de 0's y 1's tanto en train, validación como en test (si no, tendremos una gran probabilidad de tener en train solo ejemplos etiquetados como 0's)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forma de X_train:  (170883, 30)\n",
            "Forma de X_valid:  (56962, 30)\n",
            "Forma de X_test:  (56962, 30)\n",
            "\n",
            "Distribución de clases en el conjunto de entrenamiento:\n",
            "Class\n",
            "0    0.998274\n",
            "1    0.001726\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Distribución de clases en el conjunto de validación:\n",
            "Class\n",
            "0    0.998262\n",
            "1    0.001738\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Distribución de clases en el conjunto de prueba:\n",
            "Class\n",
            "0    0.99828\n",
            "1    0.00172\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separamos características (X) y la etiqueta (y) \n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# --- División Train-Valid-Test con ESTRATIFICACIÓN ---\n",
        "# Haremos 60%, 20%, 20%   (se podría hacer otra repartición)\n",
        "# Usamos stratify=y para asegurar que la proporción de clases se mantenga en ambos conjuntos.\n",
        "# Esto es fundamental en datasets desbalanceados.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(\"Forma de X_train: \", X_train.shape)\n",
        "print(\"Forma de X_valid: \", X_valid.shape)\n",
        "print(\"Forma de X_test: \", X_test.shape)\n",
        "\n",
        "print(\"\\nDistribución de clases en el conjunto de entrenamiento:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nDistribución de clases en el conjunto de validación:\")\n",
        "print(y_valid.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nDistribución de clases en el conjunto de prueba:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos crear los tensores. Observa que pasamos de un dataframe a un tensor pasando de forma intermedia a través de un numpy array (que se consigue con el método `values`). Después, creamos los Dataset y los DataLoaders correspondientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ya tenemos los datos particionados con scikit-learn\n",
        "# Ahora necesitamos transformar los datos a Tensor\n",
        "# Flujo: de dataframe --> numpy --> tensor\n",
        "X_train = torch.FloatTensor(X_train.values)\n",
        "X_test  = torch.FloatTensor(X_test.values)\n",
        "X_valid = torch.FloatTensor(X_valid.values)\n",
        "y_train = torch.FloatTensor(y_train.values)\n",
        "y_valid = torch.FloatTensor(y_valid.values)\n",
        "y_test  = torch.FloatTensor(y_test.values)\n",
        "\n",
        "# --- Creamos el Dataset de train y de test ---\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "valid_ds = TensorDataset(X_valid, y_test)\n",
        "test_ds  = TensorDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170883, 30])\n",
            "tensor([[ 1.1178,  0.2562,  0.2427,  1.0938, -0.0309, -0.4564,  0.2715, -0.1672,\n",
            "         -0.1893, -0.0810,  0.0126,  0.8481,  1.0620,  0.2132,  1.0588, -0.2885,\n",
            "         -0.2077, -0.7241, -0.7504, -0.0181,  0.0939,  0.3064, -0.1207,  0.1182,\n",
            "          0.6352, -0.3070,  0.0290,  0.0269, -0.1616, -0.8260],\n",
            "        [-1.0475,  1.4038, -1.0241, -1.0990,  2.1321,  3.7278, -1.2693, -1.3488,\n",
            "         -1.0964, -1.0854, -0.5517,  0.0869, -0.2439,  1.1204,  0.7177,  0.6170,\n",
            "         -0.5492,  0.3859,  0.1628,  0.5744, -1.1410,  0.4663,  0.0911,  1.0171,\n",
            "         -0.2585,  0.2850, -0.0537,  0.1186, -0.3472, -0.6119],\n",
            "        [ 0.1610, -3.7500, -3.5407,  0.4242, -0.9017, -1.1548,  1.8575, -0.7402,\n",
            "         -1.0276,  0.4950,  0.5841,  0.0152, -1.3410,  1.5088, -0.5715, -2.3493,\n",
            "          0.1140,  1.6737, -0.7594,  1.3491,  0.6258,  0.0335, -1.0594,  0.0560,\n",
            "          0.3031, -0.3205, -0.2341,  0.0931,  3.8564,  0.7914]])\n",
            "torch.Size([170883])\n",
            "tensor([0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "# Veamos qué forma tienen ahora los datos\n",
        "print(X_train.shape)\n",
        "print(X_train[:3])\n",
        "print(y_train.shape)\n",
        "print(y_train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Creamos los DataLoaders ---\n",
        "BATCH_SIZE = 1024\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definiendo la red\n",
        "\n",
        "Después de un pre-procesamiento sencillo, nuestros datos de entrada son vectores (uno por cada ejemplo), y las etiquetas son escalares (1s y 0s), así que estamos ante la configuración más sencilla de las posibles para ser trabajadas con una red neuronal, y un tipo de red que funciona bien con este tipo de problemas es una simple pila de capas lineales con activaciones `ReLU`.\n",
        "\n",
        "En general, en todo tipo de capas, los argumentos más habituales que tendrás que usar serán el número de neuronas en la capa, y el tipo de activación que usarán estas neuronas. En el caso anterior usamos 16 neuronas, lo que significa que esta capa usará 16 dimensiones para intentar estructurar los patrones que encuentre en los datos de entrada según la función objetivo (loss) que deba optimizar. \n",
        "\n",
        "Se puede interpretar intuitivamente que la dimensión de la capa representa cuánta libertad se permite a la red para aprender representaciones internas. Tener más unidades permite aprender representaciones más complejas, pero también aumenta la carga computacional y facilita la memorización de patrones en los datos de entrenamiento (que quizás no sean relevantes para el problema y que puede llevar al sobreajuste).\n",
        "\n",
        "Respecto a la arquitectura al trabajar con capas densas, hay dos decisiones claves que considerar:\n",
        "\n",
        "* Cuántas capas usar.\n",
        "* Cuántas unidades colocar en cada capa.\n",
        "\n",
        "Aunque no hay reglas generales para saber cómo tomar estas decisiones, sí que hay algunas razones que se pueden aprender con la experiencia y así poder extraer algún conocimiento implícito útil para el diseño de redes. Para este ejemplo, sin una justificación clara, y solo a modo de demostración de las técnicas vamos a usar la siguiente configuración:\n",
        "* Una capa oculta con 32 neuronas y función de activación ReLU\n",
        "* Otra capa oculta con 16 neuronas y función de activación ReLU\n",
        "* Una capa de salida con 1 neurona (suficiente para clasificación binaria). En teoría deberíamos usar la función *sigmoide* para que la salida esté en un rango $(0,1)$, simulando una probabilidad. Pero al igual que en práctica anterior, no añadiremos la función sigmoide por el momento. \n",
        "\n",
        "**Ejercicio:** Completa el código siguiente (reemplazando los `FIXME`) para definir la red propuesta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos las dimensiones de entrada y salida\n",
        "INPUT_FEATURES = X_train.shape[1]\n",
        "OUTPUT_FEATURES = 1 # Solo una neurona de salida para clasificación binaria\n",
        "\n",
        "# --- Arquitectura del modelo con nn.Sequential ---\n",
        "# Esta vez definimos el modelo directamente, sin usar una lista intermedia\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(FIXME, FIXME), # Capa de entrada\n",
        "    nn.FIXME(),                     # Función de activación\n",
        "    nn.Linear(FIXME, FIXME),             # Capa oculta\n",
        "    nn.FIXME(),\n",
        "    # nn.Dropout(0.2),\n",
        "    nn.FIXME(FIXME, FIXME) # Capa de salida (produce un logit)\n",
        ")\n",
        "\n",
        "model.FIXME(device) # Enviamos el modelo al dispositivo (GPU o CPU)\n",
        "\n",
        "print(\"--- Arquitectura del Modelo ---\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Arquitectura del Modelo ---\n",
            "Sequential(\n",
            "  (0): Linear(in_features=30, out_features=32, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Solución\n",
        "# Definimos las dimensiones de entrada y salida\n",
        "INPUT_FEATURES = X_train.shape[1]\n",
        "OUTPUT_FEATURES = 1 # Solo una neurona de salida para clasificación binaria\n",
        "\n",
        "# --- Arquitectura del modelo con nn.Sequential ---\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(INPUT_FEATURES, 32), # Capa de entrada\n",
        "    nn.ReLU(),                     # Función de activación\n",
        "    # nn.Dropout(0.2),               # Dropout para regularización y evitar overfitting\n",
        "    nn.Linear(32, 16),             # Capa oculta\n",
        "    nn.ReLU(),\n",
        "    # nn.Dropout(0.2),\n",
        "    nn.Linear(16, OUTPUT_FEATURES) # Capa de salida (produce un logit)\n",
        ").to(device) # Enviamos el modelo al dispositivo (GPU o CPU)\n",
        "\n",
        "model.compile()\n",
        "\n",
        "print(\"--- Arquitectura del Modelo ---\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sabemos la arquitectura del modelo, según hemos visto antes. Pero, ¿hay alguna forma de calcular el número de parámetros por capa y tener un recuento global? Se puede hacer con *torchsummary*, que se puede instalar fácilmente con `pip install torchsummary`. ¿Qué capa tiene más parámetro? ¿Cuántos parámetros hay en total?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Arquitectura Detallada del Modelo ---\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "├─Linear: 1-1                            992\n",
              "├─ReLU: 1-2                              --\n",
              "├─Linear: 1-3                            528\n",
              "├─ReLU: 1-4                              --\n",
              "├─Linear: 1-5                            17\n",
              "=================================================================\n",
              "Total params: 1,537\n",
              "Trainable params: 1,537\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "print(\"--- Arquitectura Detallada del Modelo ---\")\n",
        "summary(model,verbose=0)  \n",
        "# Usamos verbose=0 al ser la última línea de la celda, si no\n",
        "# se imprimiría dos veces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenamiento del modelo\n",
        "\n",
        "Siguiendo el mismo patrón que vimos en el ejemplo anterior, necesitamos elegir la función de pérdida (que será minimizada) y el método de optimización (que será el que busque minimizar esa función).  \n",
        "\n",
        "Como estamos ante un problema de clasificación binaria y la salida de nuestra red es una probabilidad (proporcionada por la sigmoide), usaremos `binary_crossentropy` como función de pérdida. No es la única opción viable, podríamos haber elegido, por ejemplo, `mean_squared_error`, pero en este caso `binary_crossentropy` es la mejor opción por estar trabajando con probabilidades. La entropía cruzada proviene del campo de **Teoría de la Información**, y mide la distancia entre distribuciones de probabilidad (en este caso, la distribución calculada por el predictor y la que representa la distribución _real_ proveniente de los datos de entrenamiento).\n",
        "\n",
        "Como optimizador usaremos `rmsprop`, que suele ser una buena elección. Para monitorear la evolución del aprendizaje usaremos una sola métrica, _accuracy_.\n",
        "\n",
        "### 3.1. Función de Pérdida con Pesos y Optimizador\n",
        "\n",
        "Para clasificación multiclase vimos que teníamos principalmente dos opciones para elegir la función de pérdida: `NLLLoss` y `CrossEntropyLoss`. En **clasificación binaria** tenemos algo parecido: \n",
        "* `BCELoss`: espera que las salidas del modelo sean probabilidades, es decir, valores después de una sigmoide. \n",
        "* `BCEWithLogitsLoss`: es la opción preferida en PyTorch porque opera directamente sobre los logits (las salidas sin procesar de la capa de salida). Esto tiene dos ventajas clave: \n",
        "  * *Estabilidad numérica*: La combinación de sigmoid y log en una sola operación (log-sum-exp) es mucho más estable numéricamente (evitando overflow o underflow) que aplicar ambas por separado. \n",
        "  * *Eficiencia computacional*: Realizar una única operación es más eficiente que encadenar dos (Sigmoid + BCELoss), y la inferencia del modelo será algo más rápida al no depender de una sigmoide.\n",
        "\n",
        "No es la única opción viable, podríamos haber elegido, por ejemplo, *Mean Squared Error*, pero en este caso *Binary Cross Entropy* es la mejor opción por estar trabajando con probabilidades. Además, estas funciones también son válidas para **clasificación multi-etiqueta**, donde un ejemplo se puede clasificar en más de una clase, y por tanto tenemos una probabilidad independiente en cada etiqueta.\n",
        "\n",
        "Vamos a aprovechar también para introducir la primera técnica para manejar el desbalance: dar más peso a la clase minoritaria (fraude). Para ello, vamos a calcular un peso para la clase positiva (fraude). Por ejemplo, si hay 1000 ejemplos negativos por cada 1 positivo, el peso será cercano a 1000. Esto le dice a la función de pérdida: \"un error en un caso de fraude es 1000 veces más importante que un error en un caso normal\". Esto se consigue pasándole el peso `pos_weight` a la función de pérdida.\n",
        "\n",
        "Por último, vamos a usar esta vez el optimizador Adam, que suele ser uno de los preferidos en el Deep Learning. En este caso, a modo de ejemplo, le vamos a pasar también un valor de *learning rate* en concreto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El peso calculado para la clase 'Fraude' es: 577.88\n",
            "Esto significa que la pérdida en un ejemplo de fraude se multiplicará por este factor.\n"
          ]
        }
      ],
      "source": [
        "# --- TÉCNICA PARA DATOS DESBALANCEADOS: PONDERACIÓN DE CLASES ---\n",
        "# Calculamos el peso para la clase positiva (1: Fraude)\n",
        "# Peso = (Nº de muestras negativas) / (Nº de muestras positivas)\n",
        "weight_for_class_1 = df['Class'].value_counts()[0] / df['Class'].value_counts()[1]\n",
        "# Creamos un tensor con este valor, ya que se usará para multiplicar la pérdida\n",
        "pos_weight = torch.tensor([weight_for_class_1], dtype=torch.float32).to(device)\n",
        "\n",
        "print(f\"El peso calculado para la clase 'Fraude' es: {pos_weight.item():.2f}\")\n",
        "print(\"Esto significa que la pérdida en un ejemplo de fraude se multiplicará por este factor.\")\n",
        "\n",
        "# --- Función de Pérdida y Optimizador ---\n",
        "loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Bucle de entrenamiento\n",
        "\n",
        "Vamos a definir el bucle de entrenamiento de nuevo, esta vez refactorizando un poco el código para que no depende de variables globales. Así que proveeremos a la función de los parámetros necesarios: *modelo, número de épocas, loader del dataset, función de pérdida, y el optimizador*. También, para hace el proceso de entrenamiento más vistoso, y parecido a lo que conocerás de otros entornos como Keras, vamos a introducir una barra de progreso, empleando la librería `tqdm`. Esto es completamente opcional, pero puede hacer que la función se vea mejor. Esta librería necesita crear un objeto que englobe a un iterador, convirtiéndose en el nuevo objeto iterador. Automáticamente irá midiendo el tiempo entre llamadas al iterador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# definimos de nuevo el accuracy para poder mostrar el progreso\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)  \n",
        "    return (preds == yb).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usaremos una librería que permite visualizar mejor el progreso del entrenamiento\n",
        "# se instala con: pip install tqdm\n",
        "# tan solo hay que pasarle un iterador\n",
        "from tqdm import tqdm \n",
        "\n",
        "def train_model(model, epochs, train_loader, valid_loader, loss_fn, optimizer):  \n",
        "    # --- Bucle principal ---\n",
        "    for epoch in range(epochs):\n",
        "        # --- Fase de Entrenamiento ---\n",
        "        model.train() # Ponemos el modelo en modo de entrenamiento\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            loss_acum, accu_acum = 0, 0            \n",
        "            for x, y in tepoch:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(x)\n",
        "                loss = loss_fn(preds, y.unsqueeze(1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # calculamos métricas\n",
        "                loss_acum += loss.item()\n",
        "                accu=accuracy(preds,y)\n",
        "                accu_acum += accu              \n",
        "                \n",
        "                # añadimos a tqdm qué imprimir\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")    \n",
        "                tepoch.set_postfix(loss=loss.item(),accuracy=accu)\n",
        "        \n",
        "        # añadimos la media de la pérdida para mostrarla después\n",
        "        avg_train_loss = loss_acum / len(train_loader)        \n",
        "        avg_train_accu = accu_acum / len(train_loader)\n",
        "\n",
        "        # --- Fase de Validación ---\n",
        "        model.eval() # Ponemos el modelo en modo de evaluación\n",
        "        loss_acum, accu_acum = 0, 0\n",
        "        with torch.no_grad(): # Desactivamos el cálculo de gradientes\n",
        "            for x, y in valid_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                preds = model(x)\n",
        "                loss = loss_fn(preds, y.unsqueeze(1))\n",
        "                loss_acum += loss.item()\n",
        "                accu_acum += accuracy(preds,y)\n",
        "        \n",
        "        # añadimos la media de la pérdida para mostrarla después\n",
        "        avg_val_loss = loss_acum / len(valid_loader)        \n",
        "        avg_val_accu = accu_acum / len(valid_loader)\n",
        "        \n",
        "        print(f\"Resumen época [{epoch}/{epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Train Accuracy: {avg_train_accu:.4f}, Validation Accuracy: {avg_val_accu:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Entrenamiento finalizado ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 167/167 [00:01<00:00, 146.09batch/s, accuracy=0.999, loss=0.252]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [0/10], Train Loss: 0.9278, Validation Loss: 1.9256, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 167/167 [00:01<00:00, 145.78batch/s, accuracy=0.998, loss=0.133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [1/10], Train Loss: 0.3535, Validation Loss: 3.0103, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 167/167 [00:01<00:00, 145.02batch/s, accuracy=0.999, loss=0.117] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [2/10], Train Loss: 0.2586, Validation Loss: 3.5889, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 167/167 [00:01<00:00, 146.13batch/s, accuracy=1, loss=0.074]     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [3/10], Train Loss: 0.2187, Validation Loss: 4.3921, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 167/167 [00:01<00:00, 144.10batch/s, accuracy=0.999, loss=0.0672]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [4/10], Train Loss: 0.2021, Validation Loss: 4.7693, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 167/167 [00:01<00:00, 144.53batch/s, accuracy=0.997, loss=0.0748]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [5/10], Train Loss: 0.1840, Validation Loss: 4.9649, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 167/167 [00:01<00:00, 145.66batch/s, accuracy=0.996, loss=0.139] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [6/10], Train Loss: 0.1688, Validation Loss: 5.0819, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 167/167 [00:01<00:00, 144.53batch/s, accuracy=0.997, loss=3.26]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [7/10], Train Loss: 0.1601, Validation Loss: 5.6406, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 167/167 [00:01<00:00, 143.04batch/s, accuracy=0.999, loss=0.0613]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [8/10], Train Loss: 0.1479, Validation Loss: 5.6767, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 167/167 [00:01<00:00, 145.36batch/s, accuracy=0.998, loss=0.0453]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen época [9/10], Train Loss: 0.1389, Validation Loss: 6.1628, Train Accuracy: 0.9983, Validation Accuracy: 0.9983\n",
            "\n",
            "--- Entrenamiento finalizado ---\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "train_model(model,epochs,train_loader,valid_loader,loss_func,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ejercicio:** Responde a las siguientes preguntas:\n",
        "* ¿Cuál es la tendencia de la pérdida en el conjunto de validación comparado con el del entrenamiento?\n",
        "* ¿Cómo se compara la tendencia anterior con el accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Estas gráficas muestras que la _pérdida_ de entrenamiento decrece en cada epoch, y que el _accuracy_ en entrenamiento crece, algo que indica que el procedimiento de optimización está funcionando adecuadamente (sobre todo respecto a la función de pérdida). Pero en este caso observamos que no ocurre lo mismo con la validación, que empiezan a empeorar a partir de la época 4. Estamos ante un claro caso de *sobreajuste* (u *overfitting*): tras unos pocos pasos el sistema se sobreajusta a los datos de entrenamiento, y aprende una representación que es específica a estos datos y que no puede generalizarse a otros datos.\n",
        "\n",
        "En este caso, para prevenir el overfitting podríamos parar el entrenamiento tras las 3 primeras iteraciones. Más adelante veremos algunas otras técnicas para mitigar este efecto, pero por ahora nos contentaremos con este procedimiento que, aún lejos de ser el mejor, evita este problema ahora mismo.\n",
        "\n",
        "Vamos a entrenar una nueva red desde el principio pero solo durante 4 epochs y después evaluaremos el modelo sobre los datos de test (observa que estos datos no los hemos usado en ningún momento hasta ahora):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Solución:*\n",
        "* La pérdida en el conjunto de validación crece a lo largo de las épocas, casi linealmente. Sin embargo, la pérdida en entrenamiento se mantiene en valores bajos. Esto es un indicio claro de sobreajuste (veremos pronto cómo usar las técnicas para combatir el sobreajuste).\n",
        "* Sin embargo, el accuracy se mantiene estable tanto en entrenamiento como en validación. Esto es un indicio de que esta métrica ha saturado y el modelo ha sobreajustado demasiado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppR-AhZTrk5N"
      },
      "source": [
        "## 4. Evaluando el modelo\n",
        "\n",
        "El entrenamiento ha terminado. Ahora podemos evaluar el rendimiento del modelo en el *conjunto de test*, el cual el modelo nunca ha visto antes. Aquí vamos a evaluar otras métricas para ver qué está pasando, ya que el *accuracy* parece estar \"engañándonos\". En este caso, vamos a usar:\n",
        "\n",
        "* **Accuracy (Exactitud)**: El que ya hemos usado en entrenamiento y en validación, donde hemos conseguido valores muy altos (>99%). Esto se explica porque esta métrica es engañosa en datasets desbalanceados. Por ejemplo, un modelo que siempre predice \"No Fraude\" tendría un 99.8% de accuracy, pero sería inútil.\n",
        "* **Precision (Precisión)**: De todas las transacciones que el modelo etiquetó como fraude, ¿qué porcentaje realmente lo eran? Un valor alto significa pocas falsas alarmas (*pocos falsos positivos*).\n",
        "* **Recall (Sensibilidad)**: De todas las transacciones que eran realmente un fraude, ¿qué porcentaje logró detectar el modelo? Un valor alto indica pocos falsos negativos. Esta es la métrica clave aquí. Un recall alto es nuestro principal objetivo. Gracias a la ponderación que aplicamos a la pérdida, este valor debería ser significativamente mejor que si no hubiéramos hecho nada. \n",
        "* **F1-Score**: Es la media armónica de precisión y recall. Es una buena métrica general que balancea ambas.\n",
        "* **Matriz de Confusión:**\n",
        "  * *Verdaderos Negativos (TN)* (arriba-izquierda): Casos \"No Fraude\" correctamente clasificados. Es un número muy grande.\n",
        "  * *Falsos Positivos (FP)* (arriba-derecha): Casos \"No Fraude\" que el modelo predijo incorrectamente como \"Fraude\". Son las \"falsas alarmas\".\n",
        "  * *Falsos Negativos (FN)* (abajo-izquierda): Casos de \"Fraude\" que el modelo no detectó. Este es el error más costoso y el que más queremos minimizar.\n",
        "  * *Verdaderos Positivos (TP)* (abajo-derecha): Casos de \"Fraude\" correctamente identificados.\n",
        "\n",
        "Para un problema de fraude, el recall (sensibilidad) es a menudo la métrica más importante. Queremos identificar la mayor cantidad posible de fraudes reales, incluso si eso significa tener algunas falsas alarmas (falsos positivos). Hay otros tipos de problemas, como la videovigilancia, donde buscamos lo contrario, reducir los falsos positivos, pero éste no es nuestro caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Métricas de Evaluación ---\n",
            "Accuracy: 0.9788\n",
            "\n",
            "Informe de Clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Fraude (0)       1.00      0.98      0.99     56864\n",
            "   Fraude (1)       0.07      0.91      0.13        98\n",
            "\n",
            "     accuracy                           0.98     56962\n",
            "    macro avg       0.53      0.94      0.56     56962\n",
            " weighted avg       1.00      0.98      0.99     56962\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# --- Evaluación del Modelo ---\n",
        "model.eval() # Ponemos el modelo en modo de evaluación\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): # No necesitamos calcular gradientes durante la evaluación\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = model(x)\n",
        "\n",
        "        # Convertir los logits de salida a probabilidades con sigmoide, para una mejor clasificación\n",
        "        probs = torch.sigmoid(preds)\n",
        "        # Convertir probabilidades a predicciones binarias (0 o 1) con un umbral de 0.5\n",
        "        preds = (probs > 0.5).float()\n",
        "        # Alternativamente, para ir más rápido, podríamos haber usado los logits\n",
        "        # Si es mayor que 0, positivo, menor que cero, negativo.\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy()) # con .cpu() traemos los datos desde la GPU\n",
        "        all_labels.append(y.cpu().numpy())\n",
        "\n",
        "# Convertir listas a arrays de numpy\n",
        "all_preds = np.array(all_preds).flatten()\n",
        "all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "# --- Cálculo y visualización de las métricas ---\n",
        "print(\"\\n--- Métricas de Evaluación ---\")\n",
        "print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
        "# El informe de clasificación nos da precision, recall y F1-score de una vez.\n",
        "print(\"\\nInforme de Clasificación:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['No Fraude (0)', 'Fraude (1)']))\n",
        "\n",
        "# --- Matriz de Confusión ---\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Fraude', 'Fraude'],\n",
        "            yticklabels=['No Fraude', 'Fraude'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que para la clase Fraude, tenemos aproximadamente un 0,07 de *precision* y un 0,91 de *recall*. ¿Es este un mal resultado? No necesariamente. Para este tipo de problemas, donde lo importante es detectar los fraudes, es un buen resultado. Indica que el modelo está teniendo muchos falsos positivos, pero eso es mejor que tener  falsos negativos. Lo puedes ver en la misma matriz de confusión, de las predicciones de Fraude, 1.196 ejemplos eran en realidad negativos, pero en el caso contrario (clasificados como negativo pero eran positivos) fueron solo 9. \n",
        "\n",
        "Este es conocido como el **problema del intercambio (trade-off)** entre precisión y recall. Es muy difícil aumentar una sin disminuir la otra. Por eso es importante entender bien el problema que estamos queriendo resolver:\n",
        "* *Problemas donde detectar es crucial*: por ejemplo, detección de enfermedades, fraudes, etc. Lo mejor es fijarse en el recall.\n",
        "* *Problemas donde demasiada detección es fatal*: por ejemplo, detección de humo en habitación, detección de armas en videovigilancia, etc (un detector que continuamente esté dando positivos lo convierte en [inútil](https://youtu.be/NITHLjbxJaE?si=34Y_UntcvYFdO9o4&t=29)). Lo mejor es fijarse en precision.\n",
        "* En casos generales: Fijarse en el F1-score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpQQrvkErk5O"
      },
      "source": [
        "## 5. Trabajo opcional propuesto\n",
        "\n",
        "* Prueba a entrenar el modelo sin la ponderación en la función de pérdida, ¿qué resultados obtienes?\n",
        "* En los modelos anteriores hemos usado 2 capas ocultas... comprueba el efecto de ampliar o reducir este número sobre la accuracy de validación y de test.\n",
        "* Cambia el número de unidades en las capas ocultas (8, 32, 64,...) y mide su efecto.\n",
        "* Cambia la función de pérdida a `MSELoss`, en vez de `BCEWithLogitsLoss`, ¿qué ocurre en el entrenamiento?\n",
        "* Modifica el modelo para que la última capa sea la función de activación `sigmoid`, y usa por tanto la función de pérdida `BCE`.\n",
        "* Mira qué ocurre al usar la activación `Tanh` en vez de `ReLU` en las capas ocultas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6XgxCprk5P"
      },
      "source": [
        "## 6. Conclusiones y recursos\n",
        "\n",
        "A pesar de haber analizado únicamente un ejemplo de introducción sin llegar a profundizar en detalles ni experimentar profundamente con los parámetros, podemos ir ya apuntando algunas conclusiones importantes que pueden ayudarnos en casos posteriores:\n",
        "\n",
        "* El trabajo de **preprocesamiento** sobre los datos es una etapa esencial para que puedas alimentar a las redes (no olvides que se alimentan de tensores).\n",
        "* Es importante que las características sigan una *distribución parecida* entre ellas.\n",
        "* Las pilas de capas lineales con activaciones `ReLU` pueden resolver una amplia variedad de problemas... así que no las olvides.\n",
        "* En los problemas de clasificación binaria, la red debería acabar en una capa densa con una sola neurona que haga uso de la activación `sigmoid`. De esta forma, la salida será un único escalar en $[0,1]$ que se puede interpretar como una probabilidad. \n",
        "* En PyTorch, solemos optar por no usar la sigmoide y basarnos en los logits que obtenemos de la neurona de salida. En este caso la función de pérdida adecuada es `BCEWithLogitsLoss`.\n",
        "* El optimizador `Adam` es normalmente una buena opción.\n",
        "* Cuidado con el *overfitting*. Asegúrate de monitorear la evolución del entrenamiento (acuérdate de preparar un conjunto de validación adicional).\n",
        "* En **datasets desbalanceados** debemos llevar precaución, porque el modelo puede sobreajustar a predicir siempre la clase predominante. Opciones a considerar cuando se trabaja con datasets desbalanceados:\n",
        "  1. Conseguir más ejemplos de la clase minoritaria (es decir, más ejemplos de Fraude). Si no es posible:\n",
        "  2. Aumentar los ejemplos de la clase minoritaria de forma artificial, a partir de los existentes (data augmentation). Si no es posible:\n",
        "  3. Recortar la clase predominante para conseguir un mayor balance. Si se prefiere no hacerlo:\n",
        "  4. Dar un peso mayor a la clase minoritaria en la función de pérdida (opción elegida en esta práctica).\n",
        "* Es importante saber el tipo de problema que queremos resolver para fijarnos en unas métricas o en otras.\n",
        "\n",
        "Recursos empleados:\n",
        "\n",
        "* [Deep Learning with Python](https://deeplearningwithpython.io/chapters/), François Chollet, Matthew Watson\n",
        "* [Credit card fraud detection with Pytorch](https://blacksuan19.dev/projects/credit-card-fraud-detection-with-pytorch/)\n",
        "* [Cienciadedatos.net](https://cienciadedatos.net/documentos/py35-redes-neuronales-python.html)\n",
        "* Gemini 2.5 Pro y GPT 5"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Practica3.4. Keras: clasificacion binaria.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

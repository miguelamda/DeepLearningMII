{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelamda/DeepLearningMII/blob/main/3_fundamentos_practicos/Practica_3.1_intro_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87c3a869-d8d0-44b1-8c49-67efb1afbe61",
      "metadata": {
        "tags": [],
        "id": "87c3a869-d8d0-44b1-8c49-67efb1afbe61"
      },
      "source": [
        "# PRÁCTICA 3.1. INTRODUCCIÓN A PYTORCH\n",
        "\n",
        "Para esta primera práctica, asumiremos que ya has revisado el contenido del módulo 2, adquiriendo así los fundamentos teóricos básicos de las redes neuronales artificiales, por lo que nos centraremos en ver cómo plasmar esos conocimientos en código ejecutable mediante PyTorch.\n",
        "\n",
        "PyTorch es un framework de código abierto para el aprendizaje profundo (Deep Learning). Destaca por su flexibilidad \"Pythónica\", su eficiencia en el cálculo de **tensores** (estructuras de datos similares a los arrays de NumPy pero optimizadas para GPU, como veremos en el siguiente módulo) y sus **grafos computacionales** dinámicos. Esta última característica simplifica la depuración y ofrece mayor libertad al construir modelos complejos. Es un proyecto de código abierto desarrollado dentro de la *Linux Foundation*, principalmente por *Meta AI* y grandes aportaciones de otras empresas como *NVIDIA*.\n",
        "\n",
        "## 0. Objetivos\n",
        "* Introducción del concepto de **Tensor**\n",
        "* Creación de una red neuronal con nivel de programación 1, solo mediante tensores\n",
        "* Introducción al bucle de entrenamiento de PyTorch.\n",
        "* Descarga de un dataset y carga en tensores.\n",
        "\n",
        "## 1. ¿Cómo funciona y cómo se instala?\n",
        "\n",
        "Pero antes, para instalar PyTorch, la forma más sencilla es visitar su página oficial (pytorch.org). Allí, selecciona tu sistema, gestor de paquetes (pip o conda), versión de Python y, lo más importante, si usarás CPU o GPU (CUDA/ROCm). La web te proporcionará el comando exacto. Si vas a usar una GPU con CUDA, se debe instalar una versión del driver de NVIDIA compatible con la GPU instalada, para ello consulta su [Compute Capability](https://developer.nvidia.com/cuda-gpus) y qué [versión del driver](https://docs.nvidia.com/datacenter/tesla/drivers/index.html) de CUDA lo soporta. Por ahora trabajaremos usando la CPU y en la próxima práctica veremos cómo usar una GPU.\n",
        "\n",
        "Alternativamente, puedes usar plataformas en la nube con todo pre-instalado, como por ejemplo las siguientes opciones con versiones gratuitas:\n",
        "\n",
        "* [*Google Colaboratory*](https://colab.google/) (Colab): mejor opción para la mayoría de usuarios, experimentos rápidos, tutoriales, proyectos de portafolio y educación. Ofrece un entorno Jupyter Notebook totalmente gratuito que proporciona acceso a GPUs (Tesla T4, L4 o A100) y TPUs de Google. Se integra bien con Google Drive (15GB si tienes cuenta de Google gratuita). La versión gratuita tiene un tiempo límite por sesión (12 horas), y el acceso a una GPU no está garantizado siempre. Existe una versión de [pago](https://colab.research.google.com/signup) (Colab Pro/Pro+) que ofrece más recursos y tiempos de ejecución más largos.\n",
        "* [*Kaggle Code*](https://www.kaggle.com/docs/notebooks) (Notebooks de Kaggle): mejor para competiciones de ciencia de datos, aprendizaje, compartir código y colaborar. Ofrece un entorno de Jupyter Notebook que proporciona acceso gratuito a GPUs P100 (algo antiguas pero funcionales) y CPUs. Kaggle es una plataforma excelente para encontrar datasets y aprender de la comunidad. Por supuesto, tiene una limitación similar a Colab para el tiempo de ejecución y recursos disponibles, aunque las especificaciones pueden variar.\n",
        "* [*Paperspace DigitalOcean*](https://www.paperspace.com/pricing): en esta plataforma puedes crear proyectos Notebook gratuitos con una GPU básica (p.ej. RTX4000), con límites de tiempo de hasta 6 horas. Al ser estudiante, puedes reclamar 200$ en créditos en DigitalOcean a través del GitHub Student Developer Pack.\n",
        "\n",
        "Una vez instalado, o usando una plataforma en la nube, podemos cargar PyTorch así:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb28c3e7-823b-40a5-b9a3-05044c5af536",
      "metadata": {
        "id": "eb28c3e7-823b-40a5-b9a3-05044c5af536"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56ee2d8-9473-4ddd-814a-f9862fa1a38e",
      "metadata": {
        "id": "e56ee2d8-9473-4ddd-814a-f9862fa1a38e",
        "outputId": "91f195bc-6119-409d-89aa-688ef76525b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# Veamos la versión que tenemos disponible\n",
        "print (torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b40279-f85f-4f67-b698-34b5d2365927",
      "metadata": {
        "id": "d8b40279-f85f-4f67-b698-34b5d2365927"
      },
      "source": [
        "## 2. Introducción a los Tensores\n",
        "\n",
        "Antes de empezar a construir este primer ejemplo, veamos con más detenimiento el concepto de **tensor**. Un tensor es un contenedor de datos, usualmente números. Por ejemplo, una matriz es un tensor de 2 dimensiones. Los tensores son generalizaciones de las matrices a un número arbitrario de dimensiones (el nombre de tensor viene en realidad de la física). En el contexto de tensores, una dimensión se denomina eje (axis).\n",
        "\n",
        "Un tensor se define por 3 atributos principales:\n",
        "* Número de ejes o rango (**rank**). Por ejemplo, una matriz o tensor 2D itene 2 ejes.\n",
        "* Forma (**shape**). Una tupla de enteros que describen cuantas dimensiones tiene el tensor en cada eje. Un escalar tiene un shape igual a (). Una matriz de 3x4 tiene un shape (3, 4).\n",
        "* Tipo de dato (**dtype**). Es el tipo de dato contenido en el tensor, por ejemplo `float32` (número real de 32 bits), `uint8` (entero sin signo de 8 bits), `float16` (número real de media precisión, 16 bits), etc. Normalmente no hay tensores de cadenas de texto, ya que los tensores viven en segmentos de memoria pre-reservados y contiguos. Puedes ver los tipos de datos soportados [aquí](https://docs.pytorch.org/docs/stable/tensors.html). Más adelante veremos cómo cambiar el tipo de dato."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8275b5e6-5b77-4432-a023-9f36ff33f4cb",
      "metadata": {
        "id": "8275b5e6-5b77-4432-a023-9f36ff33f4cb"
      },
      "source": [
        "### 2.1. Creación de tensores\n",
        "\n",
        "Existen varios métodos básicos para crear tensores de un tamaño personalizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968fd440-b520-4f9b-ac1e-3b7e8c7305a9",
      "metadata": {
        "tags": [],
        "id": "968fd440-b520-4f9b-ac1e-3b7e8c7305a9",
        "outputId": "56cd7dc0-3c90-41e9-fba0-ed89be39edfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "El shape de ones es:\n",
            "torch.Size([2, 3])\n",
            "tensor([[0.7150, 0.0523, 0.9320],\n",
            "        [0.3861, 0.8490, 0.3992]])\n",
            "tensor([[3.1416, 2.7183],\n",
            "        [1.6180, 0.0073]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tensor con ceros\n",
        "zeros = torch.zeros(2, 3)\n",
        "print(zeros)\n",
        "\n",
        "# tensor con unos\n",
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "print('El shape de ones es:')\n",
        "print(ones.shape)\n",
        "\n",
        "# tensor inicializado aleatoriamente.\n",
        "# Podemos fijar una semilla para que simpre sean los mismos:\n",
        "# torch.manual_seed(1729)\n",
        "random = torch.rand(2, 3)\n",
        "print(random)\n",
        "\n",
        "# crea un tensor desde listas de Python\n",
        "constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
        "print(constants)\n",
        "\n",
        "# existen versiones de los métodos anteriores pero con _like\n",
        "# para indicar que queremos un tensor con la misma forma que otro\n",
        "constants_zeros = torch.zeros_like(constants)\n",
        "constants.shape == constants_zeros.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17d1eec-ba36-4041-8bb2-a421653d6b95",
      "metadata": {
        "id": "b17d1eec-ba36-4041-8bb2-a421653d6b95"
      },
      "source": [
        "**Ejercicio:** crea a continuación un tensor con rango 3 y forma (2,3,2), y observa cómo se distribuyen los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf740f6-4dc0-48f4-9d43-cddb212ef3ad",
      "metadata": {
        "id": "bdf740f6-4dc0-48f4-9d43-cddb212ef3ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "60bf93e0-7a26-4499-a2f3-00aacd8e529b",
      "metadata": {
        "id": "60bf93e0-7a26-4499-a2f3-00aacd8e529b"
      },
      "source": [
        "### 2.2 Operaciones\n",
        "\n",
        "También podemos hacer operaciones aritméticas y lógicas con tensores y escalares, y con tensores y tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e626d56a-f38c-4146-9b4e-623defb3850f",
      "metadata": {
        "tags": [],
        "id": "e626d56a-f38c-4146-9b4e-623defb3850f",
        "outputId": "c1510a45-5812-4323-920e-4dfc91622796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.]])\n",
            "tensor([[4., 4., 4.],\n",
            "        [4., 4., 4.]])\n",
            "tensor([[1.4142, 1.4142, 1.4142],\n",
            "        [1.4142, 1.4142, 1.4142]])\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[6., 6.],\n",
            "        [6., 6.]])\n",
            "tensor([[12., 12., 12.],\n",
            "        [12., 12., 12.]])\n",
            "tensor([[ 2.,  4.,  8.],\n",
            "        [16., 32., 64.]])\n"
          ]
        }
      ],
      "source": [
        "# Sumar un escalar\n",
        "ones = torch.zeros(2, 3) + 1\n",
        "# Multiplicar un escalar\n",
        "twos = torch.ones(2, 3) * 2\n",
        "# Combinación\n",
        "threes = (torch.ones(2, 3) * 7 - 1) / 2\n",
        "# Potencias\n",
        "fours = twos ** 2\n",
        "sqrt2s = twos ** 0.5\n",
        "\n",
        "print(ones)\n",
        "print(twos)\n",
        "print(threes)\n",
        "print(fours)\n",
        "print(sqrt2s)\n",
        "\n",
        "# Suma de tensores\n",
        "fives = ones + fours\n",
        "print(fives)\n",
        "\n",
        "# Traspuesta de un tensor\n",
        "twos_T = twos.T\n",
        "print (twos_T)\n",
        "\n",
        "# Multiplicación matricial\n",
        "sixes = ones @ twos_T\n",
        "print (sixes)\n",
        "\n",
        "# Multiplicación elemento a elemento\n",
        "dozens = threes * fours\n",
        "print(dozens)\n",
        "\n",
        "# Potencia con tensores (por elemento)\n",
        "powers2 = twos ** torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(powers2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd1a66d",
      "metadata": {
        "id": "ddd1a66d"
      },
      "source": [
        "Cuando dos tensores tienen distinta dimensionalidad (shape), aún se pueden hacer operaciones con ellos, ya que el más pequeño se agranda automáticamente, replicando las dimensiones faltantes para ajustarse al más grande. Esto se llama **broadcasting**. Por supuesto, esto tienes algunas restricciones:\n",
        "\n",
        "* Si el shape es distinto, se añaden dimensiones de tamaño 1 por la izquierda. Por ejemplo, si el tensor es (5,3) y el otro (3), éste se extiende a (1,3)\n",
        "* Si el shape es igual pero las dimensiones son distintas, solo se puede hacer broadcasting si en las dimensiones distintas, una de ellas es de tamaño 1. En tal caso, esa dimensión se extiende hasta llegar al más grande. Por ejemplo, si tenemos (5,3) y (1,3), éste último se extiende a (5,3). Esto se hace replicando los valores que ya hay en el tensor.\n",
        "\n",
        "Por supuesto, esto no se hace directamente en memoria, sino que el broadcasting se hace forma virtual específicamente para la operación que lo requiere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2778b65",
      "metadata": {
        "id": "d2778b65",
        "outputId": "7f321986-6f7d-4d73-e961-2fac03abca41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[101, 202, 303],\n",
            "        [104, 205, 306]])\n"
          ]
        }
      ],
      "source": [
        "tensorA = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n",
        "tensorB = torch.tensor([100, 200, 300])           # Shape (3,)\n",
        "\n",
        "print (tensorA + tensorB)  # Broadcasting de B hasta conseguir shape (2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e926c54a",
      "metadata": {
        "id": "e926c54a"
      },
      "source": [
        "### 2.3 Indexado de datos\n",
        "\n",
        "Además de poder acceder a un valor en concreto dentro del tensor, también podemos obtener partes o secciones del tensor, mediante el uso de índices. Cabe recordar que los índices comienzan en 0, al igual que con otros contenedores de Python como las listas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764db1c0",
      "metadata": {
        "id": "764db1c0",
        "outputId": "f1bbee7f-1fa3-4789-94c5-9deafc45fa6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2.,  4.,  8.],\n",
            "        [16., 32., 64.]])\n",
            "tensor(2.)\n",
            "tensor([[16., 32., 64.]])\n",
            "tensor([ 4., 32.])\n",
            "tensor([[ 4.,  8.],\n",
            "        [32., 64.]])\n"
          ]
        }
      ],
      "source": [
        "# El tensor al completo\n",
        "print(powers2)\n",
        "\n",
        "# El primer elemento, en la primera fila y primera columna\n",
        "print(powers2[0,0])\n",
        "\n",
        "# Todas las filas a partir de la primera\n",
        "print(powers2[1:])\n",
        "\n",
        "# La segunda columna\n",
        "print(powers2[:,1])\n",
        "\n",
        "# A partir de la segunda columna\n",
        "print(powers2[:,1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67dc6a35",
      "metadata": {
        "id": "67dc6a35"
      },
      "source": [
        "La realidad detrás de los tensores es que una vez se crean, los datos en bruto no se modifican. Los índices se emplean como modo de acceder a los datos, da igual si se cambian las dimensiones o la forma del tensor, los datos siguen estando igual pero la forma de acceder cambia. De hecho, hay operaciones que permiten reinterpretar un tensor con una forma distinta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435a0528",
      "metadata": {
        "id": "435a0528",
        "outputId": "337d0089-1d08-4350-ef2b-71eb92270913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ],
      "source": [
        "# tensor original\n",
        "print(tensorA)\n",
        "# tensor reinterpretado con otras dimensiones\n",
        "print(tensorA.reshape(3,2))\n",
        "# la transpuesta de un tensor, distinto a lo anterior\n",
        "print(tensorA.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dadc84c0",
      "metadata": {
        "id": "dadc84c0"
      },
      "source": [
        "Finalmente, una operación muy útil (como ya veremos) es añadir o eliminar dimensiones al tensor. Siempre que sea la nueva dimensión tenga un tamaño de 1, la cantidad de datos será la misma (piénsalo, un tensor (2,2) tiene la misma cantidad de elementos que uno de (1,2,2))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86597a5",
      "metadata": {
        "id": "d86597a5",
        "outputId": "4564fece-1eb9-4c05-d439-5bbed38ce7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([1, 2, 3])\n",
            "torch.Size([2, 1, 3])\n",
            "torch.Size([2, 3, 1])\n",
            "torch.Size([2, 3, 1])\n",
            "torch.Size([2, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[ 2.,  4.,  8.],\n",
              "         [16., 32., 64.]]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(powers2.shape)\n",
        "\n",
        "# Añade una nueva dimensión en la posición indicada, en este caso en la 0\n",
        "# moviendo el resto a las posiciones 1, 2 y 3 respectivamente\n",
        "powers2ext = powers2.unsqueeze(0)\n",
        "\n",
        "# Veamos cómo queda el shape\n",
        "print(powers2ext.shape)\n",
        "# Observa a continuación dónde se añade la nueva dimensión\n",
        "print(powers2.unsqueeze(1).shape)\n",
        "print(powers2.unsqueeze(2).shape)\n",
        "print(powers2.unsqueeze(-1).shape) # añade por el final\n",
        "\n",
        "# Podemos deshacer la operación eliminando una dimensión de tamaño 1\n",
        "print(powers2ext.squeeze().shape)\n",
        "\n",
        "# Finalmente, observa cómo queda el tensor después de añadir 1 dimensión,\n",
        "# ¿dónde se han añadido los nuevos corchetes?\n",
        "powers2ext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5becd81e-ee70-4f6a-8e31-55beb1db5b2d",
      "metadata": {
        "id": "5becd81e-ee70-4f6a-8e31-55beb1db5b2d"
      },
      "source": [
        "## 3. Descarga del conjunto de datos MNIST\n",
        "\n",
        "Para este primer ejemplo, vamos a usar el clásico dataset llamado [MNIST](https://yann.lecun.com/exdb/mnist/index.html), que consiste en 70.000 imágenes en blanco y negro de dígitos escritos a mano (entre 0 y 9).\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"300px\" />\n",
        "\n",
        "Usaremos [pathlib](https://docs.python.org/3/library/pathlib.html) para usar rutas en Python y la descarga la haremos con [requests](http://docs.python-requests.org/en/master/). Aunque veremos que este dataset ya viene predefinido en otra librería de PyTorch, este primer ejemplo pretende mostrar cómo podemos descargar datasets de forma manual y mapearlos con tensores para poder entrenar modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96df725-5251-41ae-acd3-598350dab1cb",
      "metadata": {
        "tags": [],
        "id": "a96df725-5251-41ae-acd3-598350dab1cb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)  # creamos la carpeta\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists(): # evitamos descargar si ya se ha descargado\n",
        "        content = requests.get(URL + FILENAME).content  # descargamos el fichero\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63f80a3-5cfb-408f-8340-5336dea61496",
      "metadata": {
        "id": "a63f80a3-5cfb-408f-8340-5336dea61496"
      },
      "source": [
        "Este dataset está en formato **array** de **numpy**, y se ha guardado usando **pickle**, un formato específico de python para serializar datos. Normalmente usamos `X` para denominar los ejemplos (en este caso, las imágenes), e `Y` para las etiquetas (los números a los que corresponde cada imagen). El dataset también viene ya particionado en 50.000 imágenes para train, 10.000 para validación y 10.000 para test. A continuación descomprimimos el dataset (estaba en formato .gz) y de-serializamos los datos solo para train y valid (por ahora dejaremos fuera a test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fead0c3d-4f6b-446c-975f-cfc43567a5bf",
      "metadata": {
        "tags": [],
        "id": "fead0c3d-4f6b-446c-975f-cfc43567a5bf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:  # descomprimimos el fichero en f\n",
        "        ((x_train_np, y_train_np), (x_valid_np, y_valid_np), _) = pickle.load(f, encoding=\"latin-1\") # cargamos los datos serializados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7106c8-c4d1-47fb-8794-8dc61112c6b1",
      "metadata": {
        "tags": [],
        "id": "be7106c8-c4d1-47fb-8794-8dc61112c6b1",
        "outputId": "de035f03-b615-4222-edc5-97c191f24889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 784)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df8ad94-ab38-4793-ac58-09b833e733dd",
      "metadata": {
        "tags": [],
        "id": "8df8ad94-ab38-4793-ac58-09b833e733dd",
        "outputId": "7933bed1-d114-4ef5-c529-de7883111572"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_valid_np.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8b5734-7899-4e81-b1db-2f5bb648577e",
      "metadata": {
        "id": "ab8b5734-7899-4e81-b1db-2f5bb648577e"
      },
      "source": [
        "Como puedes ver, hemos cargado una matriz de 50.000 filas (cada imagen) con 784 columnas (pixeles de cada imagen). Esto es así porque cada imagen es una matriz de 28 x 28 pixeles, y se almacena de forma aplanada (`flattened`, es decir, una fila tras otra), por lo que tenemos 28x28=784 elementos en total por imagen. Veamos la primera imagen (fila 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85268540-2b9a-42d2-aa5f-7a3c02c2093a",
      "metadata": {
        "tags": [],
        "id": "85268540-2b9a-42d2-aa5f-7a3c02c2093a",
        "outputId": "1d64dd7b-9451-43b7-fe47-bba3a90a4e9a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "# Necesitamos cambiar la forma de la matriz a 28x28\n",
        "pyplot.imshow(x_train_np[0].reshape((28, 28)), cmap=\"gray\")\n",
        "try:\n",
        "    import google.colab  # si estamos en google colab, no es necesario la siguiente línea\n",
        "except ImportError:\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636e05a0-de44-4485-b5a0-30ca4d581203",
      "metadata": {
        "id": "636e05a0-de44-4485-b5a0-30ca4d581203"
      },
      "source": [
        "El número mostrado en la imagen parece un 5, comprobemos que es así consultando su etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154cb201-130d-458f-a7bb-ddcfa00b1a7b",
      "metadata": {
        "tags": [],
        "id": "154cb201-130d-458f-a7bb-ddcfa00b1a7b",
        "outputId": "5de75be0-09bc-419d-a28d-a1eda35d5528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_np[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc63a217-3f0c-4e63-ba63-4010a334d856",
      "metadata": {
        "id": "dc63a217-3f0c-4e63-ba63-4010a334d856"
      },
      "source": [
        "**Ejercicio:** Prueba a visualizar otras imágenes del conjunto de entrenamiento, así como del conjunto de validación (`x_valid`), en la siguiente celda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbc6aa8-c5a7-4e0e-b6b4-02beea45dda6",
      "metadata": {
        "tags": [],
        "id": "3cbc6aa8-c5a7-4e0e-b6b4-02beea45dda6",
        "outputId": "7ff85741-e930-47d0-c345-5a51c2512b88"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El número corresponde a un  3\n"
          ]
        }
      ],
      "source": [
        "## EJERCICIO\n",
        "\n",
        "x_ej1 = x_train_np  # 1. cambia x_train_np por x_valid_np\n",
        "y_ej1 = y_train_np  # 2. cambia y_train_np por y_valid_np\n",
        "img_ej1 = 500    # 3. cambia el número por otro\n",
        "\n",
        "pyplot.imshow(x_ej1[img_ej1].reshape((28, 28)), cmap=\"gray\")\n",
        "try:\n",
        "    import google.colab\n",
        "except ImportError:\n",
        "    pyplot.show()\n",
        "\n",
        "print('El número corresponde a un ', y_train_np[img_ej1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b91bed4-7f36-48d7-9509-615142779cfc",
      "metadata": {
        "id": "7b91bed4-7f36-48d7-9509-615142779cfc"
      },
      "source": [
        "PyTorch usa para los tensores el tipo de dato `torch.tensor` en vez de arrays de numpy, así que necesitamos convertir los datos a este formato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398edd2e-d038-46b8-a834-13727b59d310",
      "metadata": {
        "tags": [],
        "id": "398edd2e-d038-46b8-a834-13727b59d310"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# map equivale a realizar un bucle que recorra los elementos\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train_np, y_train_np, x_valid_np, y_valid_np)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05eca98b-17ba-420a-a62f-00b9ce5833dc",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "05eca98b-17ba-420a-a62f-00b9ce5833dc",
        "outputId": "4ecf4e8e-0d78-422a-eb9e-1a8e772d1212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([50000, 784])\n",
            "torch.float32\n",
            "tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "El valor mínimo y máximo de las etiquetas son  tensor(0)  y  9\n"
          ]
        }
      ],
      "source": [
        "n, c = x_train.shape\n",
        "# previsualizamos el tensor x_train (no será completo)\n",
        "print(x_train)\n",
        "# La forma del tensor\n",
        "print(x_train.shape)\n",
        "# El tipo de dato en el tensor\n",
        "print(x_train.dtype)\n",
        "# Previsualizamos Y_train\n",
        "print(y_train)\n",
        "# Con .item() sacamos el valor de un tensor (mira la diferencia entre min y max)\n",
        "print('El valor mínimo y máximo de las etiquetas son ',y_train.min(), ' y ', y_train.max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91355c71",
      "metadata": {
        "id": "91355c71"
      },
      "source": [
        "**Ejercicio:** Accede al primer tensor que corresponde a la primera imagen del conjunto de entrenamiento `x_train`. A continuación, calcula el tipo de dato con el que está codificado, el valor mínimo, el máximo, y el tamaño `size` del tensor. Puedes ver la solución en la celda oculta abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c2476f",
      "metadata": {
        "id": "95c2476f"
      },
      "outputs": [],
      "source": [
        "# Tensor de la primera imagen de x_train\n",
        "x0 = FIXME\n",
        "# Tipo de dato\n",
        "print(x0.FIXME)\n",
        "# Mínimo, máximo y size\n",
        "print(x0.FIXME)\n",
        "print(x0.FIXME)\n",
        "print(x0.FIXME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c30ecf",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "cellView": "form",
        "id": "02c30ecf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Solución\n",
        "# Tensor de la primera imagen de x_train\n",
        "x0 = x_train[0]\n",
        "# Tipo de dato\n",
        "print(x0.dtype)\n",
        "# Mínimo, máximo y size\n",
        "print(x0.min().item())\n",
        "print(x0.max().item())\n",
        "print(x0.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47eac05-8130-4682-93dd-4dc41653a281",
      "metadata": {
        "id": "b47eac05-8130-4682-93dd-4dc41653a281"
      },
      "source": [
        "Podemos ver que el tipo de dato con el que codificamos los valores de los píxeles de las imágenes de entrada es el `float32`: punto flotante de 32 bits. En realidad, cualquier formato de compresión de imágenes tratará cada pixel como un `uint8`, es decir, valores entre 0 y 255. Sin embargo, el dataset que hemos descargado es una serialización preparada para ser usado en machine learning. Por tanto, ya hemos recibido los valores previamente normalizados en el rango $[0,1]$.\n",
        "\n",
        "## 4. Primera red neuronal básica con tensores\n",
        "\n",
        "Ahora que ya sabes cómo operar de forma básica con tensores, vamos a construir una primera red neuronal, que será un simple modelo lineal, y veremos cómo PyTorch incluye herramientas para hacer la retropropagación de gradientes.\n",
        "\n",
        "### 4.1. Definición del modelo\n",
        "Vamos a hacer un primer modelo lineal en Pytorch con simples operaciones tensoriales, usando tensores para representar los pesos `weights` y el bias `bias`: `y_hat = X*weights+bias`, donde `y_hat` será la predicción de la variable `Y`. En concreto, el modelo tan solo tendrá:\n",
        "* una capa de entrada: 784 neuronas, ya que cada imagen de entrada tiene 28x28=784 píxeles\n",
        "* una capa de salida: 10 neuronas, uno para cada posible clase o categoría, en este caso para cada posible dígito del 0 al 9.\n",
        "\n",
        "Los pesos por tanto serán de cada conexión entre una neurona de entrada y una de salida, es decir, 784*10.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/970798/images/5546082/ezgif.com-video-to-gif__1_.gif\" />\n",
        "\n",
        "\n",
        "Ahora vamos a añadir algo nuevo en la creación de tensores, vamos a indicar a PyTorch que hay que calcular gradientes en los tensores, con `requires_grad`. Esto hará que PyTorch grabe todas las operaciones que se hagan sobre ese tensor, para que después podamos hacer la retro-propagación de forma automática. Observa a continuación que:\n",
        "\n",
        "* para los pesos, indicamos `requires_grad` después de inicializarlo, ya que no queremos que ese paso (la división por la raíz cuadrada de 784) se registre en el gradiente. En PyTorch, añadir un `_` al final del nombre de la función indica que la operación modifica los valores de la variable.\n",
        "* para el bias sí que podemos indicar `requires_grad` en el momento de inicializarlo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c834b80-3484-4cb8-9fc4-22a9f66c62bb",
      "metadata": {
        "tags": [],
        "id": "4c834b80-3484-4cb8-9fc4-22a9f66c62bb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab037b9-0902-41bf-92cd-a7e4cdee26c9",
      "metadata": {
        "id": "bab037b9-0902-41bf-92cd-a7e4cdee26c9"
      },
      "source": [
        "**Ejercicio**: ¿Qué tipo de inicialización estamos haciendo a los pesos?\n",
        "<details>\n",
        "<summary>Click aquí para ver solución</summary>\n",
        "    Estamos haciendo la inicialización Xavier, ya que multiplicamos por `1/sqrt(n)`, con `n` el número de conexiones de entrada.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be6b089-2568-47d0-9626-90bff60c9b37",
      "metadata": {
        "id": "8be6b089-2568-47d0-9626-90bff60c9b37"
      },
      "source": [
        "### 4.2. Función de activación en capa de salida\n",
        "PyTorch se integra perfectamente con Python, de tal forma que podemos usar cualquier función estándar de este lenguaje (o un objeto \"callable\") como un modelo, ya que los gradientes se computan automáticamente siempre y cuando los datos sean tensores de PyTorch.\n",
        "\n",
        "Por otro lado, vamos a definir la función de activación de la *capa de salida*. Ya hemos visto en los temas de la asignatura que cuando trabajamos con clasificación multiclase, la función de activación más común es **Sofmax**. Como recordatorio, softmax se puede interpretar como una probabilidad de salida que indica lo probable que es que la imagen de entrada tenga cada una de las etiquetas como salida.\n",
        "\n",
        "Sin embargo, el cálculo de esta función suele ser costosa computacionalmente, ya que requiere operaciones como el exponente y la división. Es por ello que en PyTorch se suele utilizar una variante, la función de activación **[log softmax](https://www.baeldung.com/cs/softmax-vs-log-softmax)**, la cual tiene una mejor estabilidad numérica (en cuestión de precisión decimal) y es algo más ligera de calcular. Esta función se define como:\n",
        "$$\n",
        "\\text{log\\_softmax}(x_i) = \\log(\\text{softmax}(x_i)) = \\log\\left(\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\right) = x_i - \\log\\left(\\sum_j e^{x_j}\\right)\n",
        "$$\n",
        "Es decir, `log_softmax (x_i) = x_i - log(sum(exp(x_j)))`, donde para cada valor `x_i` le restamos el logaritmo de la suma de todos los valores x_j (resto de valores). Aunque estas funciones de activación ya existen en PyTorch, vamos a demostrar que podemos implementarlo a mano y además es eficiente, ya que automáticamente se creará una versión compilable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24559ea-8801-436e-b0dd-b383ec67de73",
      "metadata": {
        "tags": [],
        "id": "e24559ea-8801-436e-b0dd-b383ec67de73"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff9cbad-0da6-45aa-aa2d-f87eab879978",
      "metadata": {
        "id": "2ff9cbad-0da6-45aa-aa2d-f87eab879978"
      },
      "source": [
        "**Ejercicio opcional**: Puedes entender cómo se implementa log_softmax con `x.exp().sum(-1).log().unsqueeze(-1)` si vas añadiendo cada operación una a una sobre un tensor simple. Ejecútalo a continuación y comenta al lado de cada línea qué ha hecho:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07c9888-5e35-49df-aa82-7803f2d298b1",
      "metadata": {
        "tags": [],
        "id": "f07c9888-5e35-49df-aa82-7803f2d298b1",
        "outputId": "a1e56eef-6de2-4fee-b2d8-48c79a56c762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 1],\n",
            "        [2, 2, 2]])\n",
            "tensor([[2.7183, 2.7183, 2.7183],\n",
            "        [7.3891, 7.3891, 7.3891]])\n",
            "tensor([ 8.1548, 22.1672])\n",
            "tensor([2.0986, 3.0986])\n",
            "tensor([[2.0986],\n",
            "        [3.0986]])\n"
          ]
        }
      ],
      "source": [
        "## EJERCICIO\n",
        "sample = torch.tensor([[1,1,1],[2,2,2]])\n",
        "\n",
        "print(sample)  # el vector de entrada\n",
        "print(sample.exp())   # Aplica e^x para cada valor x en samples\n",
        "print(sample.exp().sum(-1))   # ...\n",
        "print(sample.exp().sum(-1).log())   # ...\n",
        "print(sample.exp().sum(-1).log().unsqueeze(-1))    #..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a51558f",
      "metadata": {
        "id": "6a51558f"
      },
      "source": [
        "Finalmente, definimos el modelo lineal, el cual es simplemente la multiplicación matricial (operador `@`) entre la entrada y los pesos, más el bias, pasado por la función de activación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891f07ce",
      "metadata": {
        "id": "891f07ce"
      },
      "outputs": [],
      "source": [
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd59d5f0-78e3-451f-b363-3ec66e5e1a40",
      "metadata": {
        "id": "fd59d5f0-78e3-451f-b363-3ec66e5e1a40"
      },
      "source": [
        "Vamos a usar el modelo para hacer inferencia sobre un batch de 64 elementos. Por ahora, las predicciones serán aleatorias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627fce7a-85ba-49ee-964b-567d30d49c52",
      "metadata": {
        "tags": [],
        "id": "627fce7a-85ba-49ee-964b-567d30d49c52",
        "outputId": "852cb37f-3253-47e0-a1f3-f2ba953164ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.1271, -2.8384, -1.9378, -2.6151, -2.7322, -2.1581, -2.2669, -1.7190,\n",
            "        -2.3860, -3.0034], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "batch = 64  # batch size\n",
        "\n",
        "xb = x_train[0:batch]  # a mini-batch from x\n",
        "preds = model(xb)  # predictions\n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccbfdf5-8865-4c81-9ac6-0f2ee99f7262",
      "metadata": {
        "id": "fccbfdf5-8865-4c81-9ac6-0f2ee99f7262"
      },
      "source": [
        "El tensor resultado de la predicción, `preds`, contiene también una función de gradiente (`grad_fn`). Esta función se utilizará para realizar la retropropagación.\n",
        "\n",
        "### 4.3. Función de pérdida y métrica de evaluación\n",
        "\n",
        "A continuación vamos a implementar la función de pérdida *logaritmo-verosimilitud negativa* (NLL, o negative log-likelihood), la cual se deriva de la fórmula vista en los temas para clasificación multiclase, asumiendo que solo puede haber una clase válida para cada ejemplo. Además, esta función de pérdida asume que las entradas son los valores después de aplicar el logaritmo softmax. La fórmula es:\n",
        "\n",
        "$$\n",
        "\\ell(x, y) = \\text{media}(\\{l_1, \\dots, l_N\\}), \\quad \\text{siendo } l_i = -x_{i,y_i}\n",
        "$$\n",
        "\n",
        "donde\n",
        "* N es el número de ejemplos (e.g. tamaño del batch)\n",
        "* y es un tensor con N elementos, con valores entre 0 y C-1. $y_i$ representa la clase correcta para el ejemplo $i$-ésimo.\n",
        "* x es un tensor de rango 2, con N filas y C columnas, donde:\n",
        "    * cada fila es el valor predicho por el modelo para cada ejemplo, que consiste en un vector de C valores, uno por cada clase.\n",
        "    * asumimos que **a cada fila de x le hemos aplicado log_softmax**.\n",
        "    * $x_{i,j}$ es el valor en la fila i y columna j. Es decir, es la predicción que hace el modelo de que el ejemplo $i$ pertenezca a la clase $j$. Así que, $x_{i,y_i}$ es la probabilidad con la que cree el modelo que el ejemplo i pertenezca a la clase correcta (es decir, cómo de bien lo ha hecho para ese ejemplo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3e7545-1995-4a88-b2f3-ff91f8afdc97",
      "metadata": {
        "id": "5d3e7545-1995-4a88-b2f3-ff91f8afdc97"
      },
      "outputs": [],
      "source": [
        "def nll(input, target):\n",
        "    # Para evitar problemas por si input tiene más elementos que target\n",
        "    # en vez de usar \":\" en la primera dimensión, usamos range(target.shape[0])\n",
        "    return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = nll"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b435438-70c5-461b-8370-bca1fc9bda76",
      "metadata": {
        "id": "1b435438-70c5-461b-8370-bca1fc9bda76"
      },
      "source": [
        "Hagamos una prueba rápida con el primer batch. Esto mejorará cuando entrenemos el modelo, recuerda que buscaremos reducir el valor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4446699-deec-4ff4-9325-c49e280b40e0",
      "metadata": {
        "id": "c4446699-deec-4ff4-9325-c49e280b40e0",
        "outputId": "89fecbef-d863-4029-a549-8b146226d39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3465, grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "yb = y_train[0:batch]\n",
        "print(loss_func(preds, yb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39fa6052",
      "metadata": {
        "id": "39fa6052",
        "outputId": "f9380b63-3524-4e78-9f9f-9c43fe114549"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yb.shape[0]\n",
        "preds.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d616c82-f2f2-4659-87ce-cc72dd74ab69",
      "metadata": {
        "id": "3d616c82-f2f2-4659-87ce-cc72dd74ab69"
      },
      "source": [
        "PyTorch es un entorno de bajo/medio nivel, buscando una API no demasiado extensa, dando más bien libertad al usuario para que defina sus propias funciones. Por este motivo, en PyTorch tenemos que definir nuestra propia función de **accuracy**, el cual nos dará un valor más interpetable para nosotros que la función de pérdida, pero que no sirve para entrenar un modelo. Será la suma ponderada de las veces que el modelo acierta a la hora de predecir la clase de un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d277a4-ac0e-4a2f-a639-ebabbab7b40b",
      "metadata": {
        "id": "e6d277a4-ac0e-4a2f-a639-ebabbab7b40b",
        "outputId": "a2d2afe1-1786-4db7-bbf9-4e65d035a823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1406)\n"
          ]
        }
      ],
      "source": [
        "def accuracy(out, yb):\n",
        "    # la predicción es la clase del valor más alto\n",
        "    # como tenemos un batch, hay que aplicarlo a través de la dimensión de cada ejemplo\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()\n",
        "\n",
        "print(accuracy(preds, yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200b6d63-a347-4d72-9cf3-c7848dd97b63",
      "metadata": {
        "id": "200b6d63-a347-4d72-9cf3-c7848dd97b63"
      },
      "source": [
        "Efectivamente, al ser un modelo aleatorio, y al haber 10 clases, esperamos obtener alrededor de un 1/10 = 0.10 = 10% de precisión. Esto se conoce como el \"random baseline\". Por cierto, es posible que cada vez que ejecutes este notebook desde cero obtengas un valor distinto, ¿por qué?\n",
        "\n",
        "Vamos a mejorarlo entrenando el modelo.\n",
        "\n",
        "### 4.4. Bucle de entrenamiento\n",
        "\n",
        "Primero, vamos a definir un paso (**época**) del bucle de entrenamiento, dentro de una nueva función llamada `train`. Éste se estructura de la siguiente forma:\n",
        "* seleccionamos un mini-batch de tamaño `bs`\n",
        "* usamos el modelo para hacer predicciones\n",
        "* calculamos la pérdida\n",
        "* `loss.backward()` actualizará los gradientes del modelo (de `weights` y de `bias`). Si te lo preguntas, la función loss sabe cómo hacerlo porque lo hace a través de las predicciones que el modelo ha generado, cuyos gradientes ya vienen enlazados con los parámetros del modelo (como dijimos al comenzar esta sección). Recuerda, la retropropagación comienza en la función de pérdida, y de ahí vamos propagando los gradientes.\n",
        "* Usamos los gradientes para actualizar los parámetros (`weights` y `bias`). Est aactualización no debe registrarse como operaciones a registrar para propagación de gradientes, por lo que usamos el contexto `torch.no_grad()`.\n",
        "* Finalmente, reiniciamos los gradientes a cero con `.grad.zero_()` para la siguiente iteración. Si no se hace esto, estaríamos siempre acumulando los gradientes entre las iteraciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960a3d1f-0568-4258-bade-0a778921076f",
      "metadata": {
        "id": "960a3d1f-0568-4258-bade-0a778921076f"
      },
      "outputs": [],
      "source": [
        "bs = 64   # tamaño del batch\n",
        "lr = 0.5  # factor de aprendizaje (learning rate)\n",
        "\n",
        "def train():\n",
        "    steps = (n - 1) // bs + 1  # número de iteraciones en una época\n",
        "    lossAcum = 0  # variables para acumular loss y acc durante la época\n",
        "    accuAcum = 0  # y mostrar el resultado al final de la época\n",
        "\n",
        "    for i in range(steps):   # bucle para cada mini-batch\n",
        "        start_i = i * bs       # donde empieza el mini-batch\n",
        "        end_i = start_i + bs   # donde finaliza el mini-batch\n",
        "        xb = x_train[start_i:end_i]  # extraemos ejemplos\n",
        "        yb = y_train[start_i:end_i]  # extraemos etiquetas\n",
        "        pred = model(xb)             # el modelo genera predicciones\n",
        "        loss = loss_func(pred, yb)   # calculamos la pérdida\n",
        "\n",
        "        loss.backward()              # retro-propagamos los gradientes\n",
        "\n",
        "        with torch.no_grad():        # actualizamos pesos (no registramos gradientes!)\n",
        "            global weights\n",
        "            global bias\n",
        "            weights -= weights.grad * lr    # regla de actualización simple, sin momentum\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()     # reseteamos los gradientes a cero\n",
        "            bias.grad.zero_()\n",
        "\n",
        "        lossAcum += loss.item()   # sacamos el valor fuera del tensor\n",
        "        accuAcum += accuracy(pred,yb).item()  # sacamos el valor fuera del tensor\n",
        "\n",
        "    # Imprimimos la media de loss y acc en toda la época\n",
        "    print('Entrenamiento - Loss: {:.4f} Accuracy: {:.4f}'.format(lossAcum/steps, accuAcum/steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91537fb9",
      "metadata": {
        "id": "91537fb9"
      },
      "source": [
        "Después de una época, comprobamos cómo se comporta el modelo con el conjunto de validación. Esto lo hacemos con una nueva función más simple que la anterior, donde no hacemos la fase de backward, sino solo de forward.\n",
        "\n",
        "**Ejercicio:** A continuación, corrije los FIXME que veas en el código. Si te atascas, mira la solución abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1c0ddb",
      "metadata": {
        "id": "ec1c0ddb"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    lossAcum = 0\n",
        "    accuAcum = 0\n",
        "\n",
        "    m = y_valid.FIXME   # tamaño del conjunto validación\n",
        "    steps = (m - 1) // bs + 1  # número de iteraciones en una época\n",
        "\n",
        "    with torch.FIXME(): # a continuación no registramos gradientes\n",
        "        for i in range(FIXME):   # bucle para cada mini-batch\n",
        "            start_i = i * bs       # donde empieza el mini-batch\n",
        "            end_i = start_i + bs   # donde finaliza el mini-batch\n",
        "            xb = x_valid[start_i:end_i]  # extraemos ejemplos\n",
        "            yb = y_valid[start_i:end_i]  # extraemos etiquetas\n",
        "            pred = model(FIXME)             # el modelo genera predicciones\n",
        "            loss = loss_func(FIXME, FIXME)   # calculamos la pérdida\n",
        "\n",
        "    #FIXME: añade con la identación correcta lossAcum += loss.item()\n",
        "    #FIXME: añade con la identación correcta accuAcum += accuracy(pred,yb).item()\n",
        "\n",
        "    print('Validación - Loss: {:.4f} Accuracy: {:.4f}'.format(lossAcum/steps, accuAcum/steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ca7c1f",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "cellView": "form",
        "id": "b4ca7c1f"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#Solución\n",
        "\n",
        "def validate():\n",
        "    lossAcum = 0\n",
        "    accuAcum = 0\n",
        "\n",
        "    m = y_valid.shape[0]   # tamaño conjunto validación\n",
        "    steps = (m - 1) // bs + 1  # número de iteraciones en una época\n",
        "\n",
        "    with torch.no_grad(): # a continuación no registramos gradientes\n",
        "        for i in range(steps):   # bucle para cada mini-batch\n",
        "            start_i = i * bs       # donde empieza el mini-batch\n",
        "            end_i = start_i + bs   # donde finaliza el mini-batch\n",
        "            xb = x_valid[start_i:end_i]  # extraemos ejemplos\n",
        "            yb = y_valid[start_i:end_i]  # extraemos etiquetas\n",
        "            pred = model(xb)             # el modelo genera predicciones\n",
        "            loss = loss_func(pred, yb)   # calculamos la pérdida\n",
        "\n",
        "            lossAcum += loss.item()   # sacamos el valor fuera del tensor\n",
        "            accuAcum += accuracy(pred,yb).item()  # sacamos el valor fuera del tensor\n",
        "\n",
        "    print('Validación - Loss: {:.4f} Accuracy: {:.4f}'.format(lossAcum/steps, accuAcum/steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "909f03b2",
      "metadata": {
        "id": "909f03b2"
      },
      "source": [
        "Y finalmente, el bucle de entrenamiento sería llamar a train y a valid en cada época. Si ejecutas más de una vez esta celda, estarías haciendo más épocas sobre el modelo, además de las ya hechas anteriormente (sería acumulativo). Si quieres empezar de cero otra vez, simplemente ejecuta la celda justo debajo, en la cual definimos de nuevo el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902ff53c",
      "metadata": {
        "id": "902ff53c",
        "outputId": "270a3bba-99bb-4ec1-ce36-20fedac39017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época: 0\n",
            "Entrenamiento - Loss: 0.3870 Accuracy: 0.8894\n",
            "Validación - Loss: 0.3112 Accuracy: 0.9113\n",
            "Época: 1\n",
            "Entrenamiento - Loss: 0.3088 Accuracy: 0.9131\n",
            "Validación - Loss: 0.2918 Accuracy: 0.9175\n",
            "Época: 2\n",
            "Entrenamiento - Loss: 0.2954 Accuracy: 0.9167\n",
            "Validación - Loss: 0.2848 Accuracy: 0.9205\n",
            "Época: 3\n",
            "Entrenamiento - Loss: 0.2879 Accuracy: 0.9188\n",
            "Validación - Loss: 0.2814 Accuracy: 0.9223\n"
          ]
        }
      ],
      "source": [
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Época: {}'.format(epoch))\n",
        "    train()\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1138ab",
      "metadata": {
        "id": "6d1138ab"
      },
      "outputs": [],
      "source": [
        "# Ejectua las siguientes líneas SOLO si necesitas reiniciar el modelo\n",
        "# para comenzar un entrenamiento desde cero otra vez\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99936fd2-8560-4098-8418-a8581260c96a",
      "metadata": {
        "id": "99936fd2-8560-4098-8418-a8581260c96a"
      },
      "source": [
        "Vemos como han mejorado los valores a lo largo de las épocas, aunque parece que se queda estancado. Si embargo, la precisión conseguida es casi perfecta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ed4ad6",
      "metadata": {
        "id": "51ed4ad6"
      },
      "source": [
        "## 5. Conclusiones\n",
        "\n",
        "En esta práctica has trabajado principalmente con el concepto de tensor, carga de datos y entrenamiento de un modelo lineal. Sin embargo, esta forma de trabajar directamente con tensores no es la que se suele utilizar, PyTorch da herramientas para trabajar a un mayor nivel de abstracción. Sí, lo hemos hecho por el camino difícil, pero de esta forma la siguiente práctica te resultará más sencilla de seguir. Considera hacer una comparativa ente el código de esta práctica y el de la siguiente para ver las diferencias que se introducirán. En definitiva, gracias a esta práctica has podido trabajar directamente con los tipos de datos básicos de PyTorch, los tensores.\n",
        "\n",
        "## 6. Bibliografía\n",
        "\n",
        "* [Tutorial de PyTorch](https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html), por Jeremy Howard (creador de fast.ai).\n",
        "* [Introducción a los tensores de PyTorch](https://docs.pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html).\n",
        "* [Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch), Eli Stevens, Luca Antiga, Thomas Viehmann."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
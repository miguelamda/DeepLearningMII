{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 6.3: IMPLEMENTACIÓN DE UN TRANSFORMER\n",
    "\n",
    "\n",
    "En esta práctica vamos a implementar y comparar una arquitectura Transformer Encoder hecha \"a mano\" frente a la implementación nativa de PyTorch, aplicando el modelo al dataset IMDB.\n",
    "\n",
    "## 0. Objetivos \n",
    "1.  **Learned Positional Embeddings:** Sustitución de las funciones seno/coseno por embeddings aprendibles.\n",
    "2.  **Arquitectura Modular:** Atención Multi-Cabezal y FeedForward.\n",
    "3.  **Comparativa:** Switch para alternar entre `ManualTransformerEncoder` y `nn.TransformerEncoder`.\n",
    "4.  **Aplicación:** Clasificación binaria (Positivo/Negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Configuración de dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "VOCAB_SIZE = 10000   # Tamaño de vocabulario supuesto\n",
    "MAX_LEN = 200        # Longitud fija de secuencia (truncado/padding)\n",
    "NUM_SAMPLES = 1000   # Número de reviews para entrenar (en IMDB son 25k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos (Integra tu código aquí)\n",
    "\n",
    "**Ejercicio**: Vuelve a cargar los datos IMDB, tal y como hicimos en la práctica 6.1. Necesitarás traer la función que carga el vocabulario, el tokenizer (text_to_indices), y cargar los datos de IMDB.\n",
    "\n",
    "**Requisitos de tus datos:** Define las siguientes variables\n",
    "* `x_train`: Tensor de forma `(num_samples, max_len)` con índices de palabras.\n",
    "* `y_train`: Tensor de forma `(num_samples)` con etiquetas 0 o 1.\n",
    "* `x_test`: Tensor de forma `(num_samples, max_len)` con índices de palabras de test.\n",
    "* `y_test`: Tensor de forma `(num_samples)` con etiquetas 0 o 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade aquí tu código. Básate en la práctica 6.1 para ello.\n",
    "# Usa los siguientes valores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Construimos el vocabulario (las 1000 palabras más comunes)\n",
    "def build_vocab(texts, max_words=1000):    \n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        # Tokenización muy básica por espacios y minúsculas\n",
    "        counter.update(text.lower().split())\n",
    "    \n",
    "    # Mapeo palabra -> índice (comenzando en 1, 0 reservado para padding)\n",
    "    vocab = {word: i+1 for i, (word, _) in enumerate(counter.most_common(max_words))}\n",
    "    return vocab\n",
    "\n",
    "# Esto convierte las cadenas en listas de índices enteros.\n",
    "def text_to_indices(texts, vocab, maxlen=20):\n",
    "    indices_list = []\n",
    "    for text in texts:\n",
    "        # Convertir palabras a índices, ignorar las desconocidas\n",
    "        seq = [vocab.get(word.lower(), 0) for word in text.split()]\n",
    "        #seq = [idx for idx in seq if idx != 0] # Quitamos desconocidas (opcional)\n",
    "        \n",
    "        # Padding (Pre-padding)\n",
    "        if len(seq) < maxlen:\n",
    "            seq = [0] * (maxlen - len(seq)) + seq\n",
    "        else: # Truncating \n",
    "            seq = seq[:maxlen]\n",
    "            \n",
    "        indices_list.append(seq)\n",
    "    return np.array(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando IMDB desde Hugging Face:\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar el dataset desde Hugging Face\n",
    "print(\"Descargando IMDB desde Hugging Face:\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# El dataset tiene formato diccionario: {'train':, 'test':, 'unsupervised':}\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "# 2. Construir Vocabulario (Tokenización manual simple)\n",
    "vocab = build_vocab(train_texts, VOCAB_SIZE-1)\n",
    "\n",
    "# 3. Función para vectorizar (Texto -> Índices)\n",
    "x_train = text_to_indices(train_texts, vocab, MAX_LEN)\n",
    "x_test = text_to_indices(test_texts, vocab, MAX_LEN)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados. Shape X: (25000, 200), Shape y: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Crear DataLoader estándar de PyTorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(torch.tensor(x_train).long(), torch.tensor(y_train).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).long(), torch.tensor(y_test).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Datos cargados. Shape X: {x_train.shape}, Shape y: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Componentes del Transformer  \n",
    "\n",
    "Aquí definimos los bloques fundamentales de la arquitectura Transformer, solo en la parte de encoder. Hemos simplificado el **Positional Encoding** para usar embeddings aprendidos (`nn.Embedding`) en lugar de las funciones sinusoidales fijas.\n",
    "\n",
    "La siguiente clase gestiona la etapa de *representación inicial*, un paso crucial debido a que la arquitectura Transformer procesa toda la secuencia en paralelo y, por tanto, carece de un sentido inherente del orden (a diferencia de las redes recurrentes o RNNs). Para solucionar esto, el modelo necesita fusionar la información semántica (qué es la palabra) con la información posicional (dónde está). Esto se logra mediante la suma vectorial elemento a elemento de dos embeddings: el `token_emb`, que captura el significado de la palabra, y el `pos_emb`, que codifica su índice dentro de la frase.\n",
    "\n",
    "En este caso usaremos *Learned Positional Embeddings* en lugar de las funciones fijas de seno y coseno propuestas originalmente por Vaswani et al (ver Tema 7.2). Al utilizar nn.Embedding(max_len, d_model), tratamos las posiciones de manera análoga a las palabras: el modelo inicializa vectores aleatorios para la posición 0, para la posición 1, etc., y aprende sus valores óptimos mediante backpropagation durante el entrenamiento. Esto simplifica la arquitectura y permite que el modelo adapte su representación del espacio específicamente para la tarea de clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerInput(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa de entrada: Suma Token Embeddings + Positional Embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Embedding de las palabras/tokens\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        # Positional Embedding aprendible (no sinusoidal)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        b, seq_len = x.shape\n",
    "        \n",
    "        # Crear vector de posiciones [0, 1, 2, ...]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(b, -1)\n",
    "        \n",
    "        # Sumar y aplicar dropout\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `MultiHeadAttention` (abajo) constituye el núcleo del mecanismo de auto-atención. En lugar de realizar una única función de atención, proyecta la entrada en tres espacios diferentes **(Query, Key, Value)** y divide estas matrices en múltiples \"cabezas\" independientes. Esto permite al modelo capturar simultáneamente diferentes tipos de relaciones (como dependencias sintácticas vs. semánticas) en distintas posiciones de la secuencia. Vamos a implementar esto mediante operaciones matriciales: calcula los pesos de atención (Scaled Dot-Product), aplica la máscara para ignorar el padding y finalmente concatena los resultados de todas las cabezas para proyectarlos de vuelta a la dimensión original.\n",
    "\n",
    "Por su parte, la clase `FeedForward` es una red neuronal densa que se aplica \"posición a posición\" (es decir, a cada palabra por separado e idénticamente). Funciona como una capa de procesamiento que toma la información contextualizada por la atención y la transforma: expande la dimensión de los datos (típicamente multiplicando d_model por 4), aplica una no-linealidad (ReLU) y luego los comprime de nuevo. Su objetivo es añadir profundidad y capacidad de cómputo no lineal al bloque, permitiendo que el modelo procese las características extraídas por la atención antes de pasar al siguiente bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación manual de la Atención Multi-Cabezal.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # Dimensión de los vectores en el modelo\n",
    "        self.d_model = d_model\n",
    "        # Dimensión de cada cabeza\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Q, K, V son del tamaño del modelo, pero\n",
    "        # luego lo dividiremos en cabezas\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Proyecciones y reshape para separar cabezas. La forma final es (Batch, Heads, Seq, d_k)\n",
    "        # Al conseguir esa forma, podemos hacer la multiplicación de todas las cabezas a la vez\n",
    "        # con solo un matmul, lo cual es más eficiente\n",
    "        Q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Fíjate que la entrada x pasa en paralelo por Q, K y V, ahora multplicamos todo\n",
    "        # Atención escalar escalada\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Cuando la frase es más corta que la longitud de entrada, podemos hacer padding,\n",
    "        # dejando los huecos con valores muy pequeños, para que la atención no asocie las\n",
    "        # palabras con los huecos\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Unificar cabezas\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementaciones del Encoder: Manual vs Nativa\n",
    "\n",
    "Aquí creamos dos clases distintas:\n",
    "1.  **Manual:** Usa las clases anteriores, demostrando que podemos crear un modelo Transformer de forma manual. La implementación manual (`ManualTransformerEncoder`) reconstruye la arquitectura fielmente siguiendo el esquema original de Vaswani et al. La clase ManualEncoderLayer define la unidad fundamental del modelo, orquestando las dos subcapas principales: la atención y la red feed-forward. Un detalle crítico en este código es la aplicación explícita de las conexiones residuales y la normalización (el patrón norm(x + subcapa)), que son esenciales para evitar el desvanecimiento del gradiente en redes profundas. El encoder completo no es más que una pila secuencial (`nn.ModuleList`) de estas capas idénticas, donde la salida de una sirve directamente como entrada de la siguiente, refinando progresivamente la representación de la secuencia.\n",
    "\n",
    "\n",
    "2.  **Nativa:** Usa `nn.TransformerEncoder` de PyTorch, que ya implementa todo lo anterior, y está optimizada en C++/CUDA. `NativeTransformerEncoder` delega toda esta lógica en las clases optimizadas de PyTorch. Al utilizar nn.TransformerEncoderLayer, accedemos a kernels escritos en C++ y CUDA que son significativamente más rápidos y eficientes en el uso de memoria que nuestra versión en Python puro. Un punto técnico a destacar aquí es el parámetro batch_first=True; por defecto, los Transformers en PyTorch esperan los datos en formato (Seq, Batch, Dim), pero al activar esta bandera forzamos al modelo a trabajar con el formato estándar (Batch, Seq, Dim), alineándolo con nuestros dataloaders y simplificando la gestión de dimensiones sin necesidad de transponer tensores manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPCIÓN 1: MANUAL ---\n",
    "\n",
    "# Una capa\n",
    "class ManualEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model) # distinto a batchnorm\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Atención + Residual + Norm\n",
    "        attn_out = self.attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        # FF + Residual + Norm\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# El Transformer\n",
    "class ManualTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ManualEncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers) # replicamos un número de capas\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:  # iteramos las capas\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPCIÓN 2: NATIVA (PYTORCH) ---\n",
    "class NativeTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # batch_first=True es clave para no transponer dimensiones\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=d_ff, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # src_key_padding_mask espera (Batch, Seq) con True donde hay padding\n",
    "        # Aquí lo pasamos simple por compatibilidad con la manual\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. El Modelo Clasificador\n",
    "\n",
    "Este modelo envuelve todo. Tiene un parámetro `use_native` en el constructor. Si es `True`, usa la capa optimizada de PyTorch; si es `False`, usa nuestra implementación manual.\n",
    "\n",
    "La clase `IMDBTransformerClassifier` actúa como el arquitecto final que integra todos los componentes para resolver la tarea específica de análisis de sentimiento. Su responsabilidad principal es gestionar la transición de una secuencia de palabras a una única predicción de clase; para ello, toma la salida enriquecida del encoder (una secuencia de vectores contextualizados) y aplica una operación de Global Mean Pooling (promedio), colapsando toda la información de la frase en un único vector representativo. Este \"vector resumen\" es el que finalmente se proyecta a través de la capa lineal (classifier_head) para obtener los logits que determinarán si la reseña es positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len, num_classes=2, use_native=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embeddings (Común)\n",
    "        self.input_layer = TransformerInput(vocab_size, d_model, max_len)\n",
    "        \n",
    "        # 2. Encoder (Seleccionable)\n",
    "        self.use_native = use_native\n",
    "        if use_native:\n",
    "            self.encoder = NativeTransformerEncoder(d_model, num_heads, d_ff, num_layers)\n",
    "        else:\n",
    "            self.encoder = ManualTransformerEncoder(d_model, num_heads, d_ff, num_layers)\n",
    "            \n",
    "        # 3. Clasificador\n",
    "        self.classifier_head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (Batch, Seq_Len)\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Paso por Encoder\n",
    "        x = self.encoder(x, mask)\n",
    "        \n",
    "        # POOLING: Promediar toda la secuencia para obtener un solo vector por review\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        logits = self.classifier_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento y Comparación\n",
    "\n",
    "Vamos a entrenar dos modelos idénticos en hiperparámetros, pero cambiando el motor del encoder. Usaremos nuestra implementación manual, y el basado en la capa ya predefinida en PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenando Modelo: MANUAL ---\n",
      "Epoch 1/10 | Loss: 0.5583 | Acc: 69.45%\n",
      "Epoch 2/10 | Loss: 0.3958 | Acc: 82.34%\n",
      "Epoch 3/10 | Loss: 0.3303 | Acc: 85.86%\n",
      "Epoch 4/10 | Loss: 0.2865 | Acc: 88.14%\n",
      "Epoch 5/10 | Loss: 0.2553 | Acc: 89.76%\n",
      "Epoch 6/10 | Loss: 0.2250 | Acc: 90.91%\n",
      "Epoch 7/10 | Loss: 0.2017 | Acc: 92.09%\n",
      "Epoch 8/10 | Loss: 0.1774 | Acc: 93.08%\n",
      "Epoch 9/10 | Loss: 0.1601 | Acc: 93.90%\n",
      "Epoch 10/10 | Loss: 0.1412 | Acc: 94.65%\n",
      "Tiempo Total (MANUAL): 42.75 segundos\n",
      "\n",
      "--- Entrenando Modelo: NATIVA (PyTorch) ---\n",
      "Epoch 1/10 | Loss: 0.5724 | Acc: 68.47%\n",
      "Epoch 2/10 | Loss: 0.4098 | Acc: 81.14%\n",
      "Epoch 3/10 | Loss: 0.3360 | Acc: 85.62%\n",
      "Epoch 4/10 | Loss: 0.2906 | Acc: 87.90%\n",
      "Epoch 5/10 | Loss: 0.2577 | Acc: 89.54%\n",
      "Epoch 6/10 | Loss: 0.2227 | Acc: 91.01%\n",
      "Epoch 7/10 | Loss: 0.1972 | Acc: 92.22%\n",
      "Epoch 8/10 | Loss: 0.1766 | Acc: 93.16%\n",
      "Epoch 9/10 | Loss: 0.1525 | Acc: 94.06%\n",
      "Epoch 10/10 | Loss: 0.1388 | Acc: 94.76%\n",
      "Tiempo Total (NATIVA (PyTorch)): 46.97 segundos\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model_name, use_native_encoder, epochs=3):\n",
    "    print(f\"\\n--- Entrenando Modelo: {model_name} ---\")\n",
    "    \n",
    "    # Instanciar modelo\n",
    "    model = IMDBTransformerClassifier(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=64,       # Dimensiones ajustadas para demostración\n",
    "        num_heads=4,\n",
    "        d_ff=128,\n",
    "        num_layers=2,\n",
    "        max_len=MAX_LEN,\n",
    "        use_native=use_native_encoder\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Métricas\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")\n",
    "        \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Tiempo Total ({model_name}): {elapsed:.2f} segundos\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# 1. Ejecutar Implementación Manual\n",
    "modelManual = train_and_evaluate(\"MANUAL\", epochs=10, use_native_encoder=False)\n",
    "\n",
    "# 2. Ejecutar Implementación Nativa (PyTorch)\n",
    "modelNative = train_and_evaluate(\"NATIVA (PyTorch)\", epochs=10, use_native_encoder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Evalúa el modelo `modelNative` sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haz un recorrido sobre el conjunto de test, y calcula el loss y el accuracy. \n",
    "# Puedes basarte en el bucle de entrenamiento, pero teniendo en cuenta lo necesario\n",
    "# para usar el modelo en modo evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5125 | Acc: 83.26%\n"
     ]
    }
   ],
   "source": [
    "# Solución\n",
    "total_loss=0\n",
    "total=0\n",
    "correct=0\n",
    "for batch_x, batch_y in test_loader:\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    modelNative.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward\n",
    "        outputs = modelNative(batch_x)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, batch_y)\n",
    "    \n",
    "        # Métricas\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "avg_loss = total_loss / len(test_loader)\n",
    "acc = 100 * correct / total\n",
    "print(f\"Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones\n",
    "\n",
    "Has visto algunos conceptos básicos para implementar Transformers. Por un lado, los Q, K y V se implementan con un solo tensor, pero luego se dividen para simular el multi-cabezal. Por otro lado, hemos visto que PyTorch ya trae capas de auto-atención, que nos puede servir para montar un modelo desde cero. Hemos evaluado las dos formas sobre IMDB, usando el tokenizador de la práctica 6.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
